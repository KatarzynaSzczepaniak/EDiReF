{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4YS-HHlMxdzr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5705f403-d313-4a8c-f048-df8e67a312f4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd drive/MyDrive/Colab \\Notebooks"
      ],
      "metadata": {
        "id": "hMHoUEE8xvX0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5a69a25e-b96e-4c3e-9a9b-b3c097889773"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Errno 2] No such file or directory: 'drive/MyDrive/Colab Notebooks'\n",
            "/content/drive/MyDrive/Colab Notebooks\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from transformers import RobertaTokenizer, RobertaModel\n",
        "import pandas as pd\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "M6Woq9BFW18k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch import cuda\n",
        "device = 'cuda' if cuda.is_available() else 'cpu'\n",
        "\n",
        "print(device)"
      ],
      "metadata": {
        "id": "4QJ_vvYdW2YT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f84ecdbd-6676-4793-9efa-c14325dba718"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cpu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data preparation"
      ],
      "metadata": {
        "id": "EqHmn_5ew5Tf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "DATASET = 'MELD'    # @param ['MELD', 'MaSaC']\n",
        "MAX_LENGTH = 128    # @param [96, 128, 256] {type: 'raw'}\n",
        "BATCH_SIZE = 16    # @param [8, 16, 32] {type: 'raw'}"
      ],
      "metadata": {
        "id": "L86nTmkQXOp-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_df = pd.read_json('data/EDiReF_train_data/MELD_train_efr.json')\n",
        "train_df[\"triggers\"] = train_df[\"triggers\"].apply(lambda lst: [np.nan if x is None else x for x in lst])\n",
        "train_df = train_df[train_df[\"triggers\"].apply(lambda lst: not any(pd.isna(x) for x in lst))]\n",
        "\n",
        "flattened_emotions = [sent for conv in train_df['emotions'] for sent in conv]\n",
        "unique_emotions = set(flattened_emotions)\n",
        "\n",
        "labels_to_ids = {k: v for v, k in enumerate(unique_emotions)}\n",
        "ids_to_labels = {v: k for v, k in enumerate(unique_emotions)}\n",
        "\n",
        "train_conversations = list(train_df['utterances'])\n",
        "train_emotions = [[labels_to_ids[emotion] for emotion in conv] for conv in list(train_df['emotions'])]\n",
        "train_triggers = list(train_df['triggers'])"
      ],
      "metadata": {
        "id": "C7KmZUkRW3tz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "val_df = pd.read_json('data/EDiReF_val_data/MELD_val_efr.json')\n",
        "val_df[\"triggers\"] = val_df[\"triggers\"].apply(lambda lst: [np.nan if x is None else x for x in lst])\n",
        "val_df = val_df[val_df[\"triggers\"].apply(lambda lst: not any(pd.isna(x) for x in lst))]\n",
        "\n",
        "val_conversations = list(val_df['utterances'])\n",
        "val_emotions = [[labels_to_ids[emotion] for emotion in conv] for conv in list(val_df['emotions'])]\n",
        "val_triggers = list(val_df['triggers'])"
      ],
      "metadata": {
        "id": "CRuTkn5tW497"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "conversations = train_conversations + val_conversations\n",
        "emotions = train_emotions + val_emotions\n",
        "triggers = train_triggers + val_triggers"
      ],
      "metadata": {
        "id": "-zM3GK2DiV34"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "def train_val_test_split(X, y1, y2, val_size = 0.2, test_size = 0.2, random_state = None):\n",
        "    X_train_val, X_test, y1_train_val, y1_test, y2_train_val, y2_test = train_test_split(\n",
        "        X, y1, y2, test_size=test_size, random_state=random_state\n",
        "    )\n",
        "\n",
        "    val_relative_size = val_size / (1 - test_size)\n",
        "\n",
        "    X_train, X_val, y1_train, y1_val, y2_train, y2_val = train_test_split(\n",
        "        X_train_val, y1_train_val, y2_train_val, test_size=val_relative_size, random_state=random_state\n",
        "    )\n",
        "\n",
        "    return (X_train, X_val, X_test, y1_train, y1_val, y1_test, y2_train, y2_val, y2_test)"
      ],
      "metadata": {
        "id": "mi-y0zznb-hG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, X_val, X_test, y1_train, y1_val, y1_test, y2_train, y2_val, y2_test = train_val_test_split(\n",
        "    conversations, emotions, triggers, test_size=0.15, val_size=0.15, random_state=2024\n",
        "    )"
      ],
      "metadata": {
        "id": "arnKxaiEeoLD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = RobertaTokenizer.from_pretrained('roberta-base')"
      ],
      "metadata": {
        "id": "C5430Zq1W-dj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def tokenize_conversation(conversations, max_length = 128):\n",
        "    input_ids = []\n",
        "    attention_masks = []\n",
        "\n",
        "    for conversation in conversations:\n",
        "        dialogue = \" [SEP] \".join(conversation)\n",
        "        encoded = tokenizer(\n",
        "            dialogue,\n",
        "            truncation = True,\n",
        "            padding = 'max_length',\n",
        "            max_length = max_length,\n",
        "            return_tensors = \"pt\"\n",
        "        )\n",
        "        input_ids.append(encoded[\"input_ids\"].squeeze(0))\n",
        "        attention_masks.append(encoded[\"attention_mask\"].squeeze(0))\n",
        "\n",
        "    return input_ids, attention_masks"
      ],
      "metadata": {
        "id": "ng_4Q4k1XJvc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def pad_labels(labels, max_length = 128):\n",
        "    padded_labels = []\n",
        "    for label_set in labels:\n",
        "        label_tensor = torch.tensor(label_set, dtype = torch.float)\n",
        "        # Pad with -1 to ignore padding tokens in the loss function\n",
        "        padded_tensor = torch.cat(\n",
        "            [label_tensor, torch.full((max_length - len(label_set),), -1)]\n",
        "        )\n",
        "        padded_labels.append(padded_tensor)\n",
        "    return padded_labels"
      ],
      "metadata": {
        "id": "kUdbsdw_XMrD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ConversationDataset(Dataset):\n",
        "    def __init__(self, input_ids, attention_masks, emotion_labels, trigger_labels):\n",
        "        self.input_ids = input_ids\n",
        "        self.attention_masks = attention_masks\n",
        "        self.emotion_labels = emotion_labels\n",
        "        self.trigger_labels = trigger_labels\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.input_ids)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return {\n",
        "            \"input_ids\": self.input_ids[idx],\n",
        "            \"attention_mask\": self.attention_masks[idx],\n",
        "            \"emotion_labels\": self.emotion_labels[idx],\n",
        "            \"trigger_labels\": self.trigger_labels[idx],\n",
        "        }"
      ],
      "metadata": {
        "id": "8pcFsfoZXNBD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_input_ids, train_attention_masks = tokenize_conversation(X_train, max_length = MAX_LENGTH)\n",
        "\n",
        "train_emotion_labels = pad_labels(y1_train, max_length = MAX_LENGTH)\n",
        "train_trigger_labels = pad_labels(y2_train, max_length = MAX_LENGTH)\n",
        "\n",
        "train_dataset = ConversationDataset(train_input_ids, train_attention_masks, train_emotion_labels, train_trigger_labels)"
      ],
      "metadata": {
        "id": "0vvyi2PbXQaL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "val_input_ids, val_attention_masks = tokenize_conversation(X_val, max_length = MAX_LENGTH)\n",
        "\n",
        "val_emotion_labels = pad_labels(y1_val, max_length = MAX_LENGTH)\n",
        "val_trigger_labels = pad_labels(y2_val, max_length = MAX_LENGTH)\n",
        "\n",
        "val_dataset = ConversationDataset(val_input_ids, val_attention_masks, val_emotion_labels, val_trigger_labels)"
      ],
      "metadata": {
        "id": "YqCBE1sMXSX9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_input_ids, test_attention_masks = tokenize_conversation(X_test, max_length = MAX_LENGTH)\n",
        "\n",
        "test_emotion_labels = pad_labels(y1_test, max_length = MAX_LENGTH)\n",
        "test_trigger_labels = pad_labels(y2_test, max_length = MAX_LENGTH)\n",
        "\n",
        "test_dataset = ConversationDataset(test_input_ids, test_attention_masks, test_emotion_labels, test_trigger_labels)"
      ],
      "metadata": {
        "id": "egHENDvpkKb3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_loader = DataLoader(train_dataset, batch_size = BATCH_SIZE, shuffle = False)\n",
        "val_loader = DataLoader(val_dataset, batch_size = BATCH_SIZE, shuffle = False)\n",
        "test_loader = DataLoader(test_dataset, batch_size = BATCH_SIZE, shuffle = False)"
      ],
      "metadata": {
        "id": "UEtU-77cXT8b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model configuration"
      ],
      "metadata": {
        "id": "F4mmyvf8wtVv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "GATE_TYPE = 'linear'  # @param ['linear', 'mlp']\n",
        "EXPERT_TYPE = 'linear' # @param ['linear', 'mlp', 'rnn']\n",
        "NUM_EXPERTS = 2 # @param {type: 'slider', min: 1, max: 8, step: 1}\n",
        "TOP_K = 2 # @param {type: 'slider', min: 1, max: 8, step: 1}"
      ],
      "metadata": {
        "id": "kueFTpHSXViM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "assert TOP_K <= NUM_EXPERTS, \"Select different values for TOP_K and NUM_EXPERTS!\""
      ],
      "metadata": {
        "id": "12upnrxiXXbM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MoEForEmotionAndTriggerClassification(nn.Module):\n",
        "    def __init__(self, gate_type = 'linear', expert_type = 'linear', num_experts, k, num_classes):\n",
        "        super(MoEForEmotionAndTriggerClassification, self).__init__()\n",
        "\n",
        "        self.roberta = RobertaModel.from_pretrained('roberta-base')\n",
        "        for param in self.roberta.parameters():\n",
        "            param.requires_grad = True  # Set to True if you want to fine-tune RoBERTa\n",
        "        hidden_size = self.roberta.config.hidden_size\n",
        "\n",
        "        gate_setup = {\n",
        "            'linear': nn.Linear(hidden_size, num_experts),\n",
        "            'mlp': nn.Sequential(nn.Linear(hidden_size, 512), nn.ReLU(), nn.Linear(512, num_experts)),\n",
        "        }\n",
        "\n",
        "        expert_setup = {\n",
        "            'linear': nn.Linear(hidden_size, hidden_size),\n",
        "            'mlp': nn.Sequential(nn.Linear(hidden_size, 512), nn.ReLU(), nn.Linear(512, hidden_size)),\n",
        "            'rnn': nn.LSTM(hidden_size, hidden_size),\n",
        "        }\n",
        "\n",
        "        self.gating_network_emotion = gate_setup[GATE_TYPE]\n",
        "        self.gating_network_trigger = gate_setup[GATE_TYPE]\n",
        "        self.experts = nn.ModuleList([expert_setup[EXPERT_TYPE] for _ in range(num_experts)])\n",
        "\n",
        "        self.emotion_classifier = nn.Linear(hidden_size, num_classes)\n",
        "        self.trigger_classifier = nn.Linear(hidden_size, 1)\n",
        "\n",
        "        self.k = k\n",
        "        self.dropout = nn.Dropout(p = 0.1)\n",
        "\n",
        "    def forward(self, input_ids, attention_mask):\n",
        "        roberta_outputs = self.roberta(input_ids = input_ids, attention_mask = attention_mask)\n",
        "        embeddings = roberta_outputs.last_hidden_state  # (batch_size, seq_len, hidden_size)\n",
        "        pooled_embeddings = embeddings.mean(dim = 1)    # (batch_size, hidden_size)\n",
        "        pooled_embeddings = self.dropout(pooled_embeddings)\n",
        "\n",
        "        # expert weights for emotion classification\n",
        "        expert_weights_emotion = self.gating_network_emotion(pooled_embeddings)  # (batch_size, num_experts)\n",
        "        expert_weights_emotion = torch.softmax(expert_weights_emotion, dim = -1)\n",
        "\n",
        "        # expert weights for trigger classification\n",
        "        expert_weights_trigger = self.gating_network_trigger(pooled_embeddings)  # (batch_size, num_experts)\n",
        "        expert_weights_trigger = torch.softmax(expert_weights_trigger, dim = -1)\n",
        "\n",
        "        # aggregate expert outputs for each task\n",
        "        combined_output_emotion = self._compute_expert_output(embeddings, expert_weights_emotion)\n",
        "        combined_output_trigger = self._compute_expert_output(embeddings, expert_weights_trigger)\n",
        "\n",
        "        combined_output_emotion = self.dropout(combined_output_emotion)\n",
        "        combined_output_trigger = self.dropout(combined_output_trigger)\n",
        "\n",
        "        emotion_logits = self.emotion_classifier(combined_output_emotion)   # (batch_size, seq_len, num_classes)\n",
        "        trigger_logits = self.trigger_classifier(combined_output_trigger).squeeze(-1)   # (batch_size, seq_len)\n",
        "\n",
        "        return emotion_logits, trigger_logits\n",
        "\n",
        "    def _compute_expert_output(self, embeddings, expert_weights):\n",
        "        batch_size, num_experts = expert_weights.size()\n",
        "        combined_output = torch.zeros_like(embeddings)\n",
        "\n",
        "        # top-k experts only are activated\n",
        "        topk_weights, topk_indices = torch.topk(expert_weights, self.k, dim = -1)\n",
        "\n",
        "        for i in range(self.k):\n",
        "            expert_idx = topk_indices[:, i]\n",
        "            weight = topk_weights[:, i].unsqueeze(-1).unsqueeze(-1)\n",
        "\n",
        "            expert_outputs = []\n",
        "            for j in range(expert_idx.size(0)):\n",
        "                expert = self.experts[expert_idx[j]]\n",
        "\n",
        "                if isinstance(expert, nn.LSTM):\n",
        "                    embedding_input = embeddings[j].unsqueeze(0)\n",
        "                    output, _ = expert(embedding_input)\n",
        "                    expert_outputs.append(output.squeeze(0))\n",
        "\n",
        "                elif isinstance(expert, nn.Linear) or isinstance(expert, nn.Sequential):\n",
        "                    output = expert(embeddings[j])\n",
        "                    expert_outputs.append(output)\n",
        "\n",
        "            expert_outputs = torch.stack(expert_outputs)\n",
        "            combined_output += weight * expert_outputs\n",
        "\n",
        "        return combined_output"
      ],
      "metadata": {
        "id": "umr0EC9_XuzD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training parameters"
      ],
      "metadata": {
        "id": "e5Bw1xqCz9bC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "LEARNING_RATE = 0.00002  # @param {type: 'slider', min: 1E-5, max: 5E-5, step: 1E-5}\n",
        "NUM_EPOCHS = 10  # @param {type: 'slider', min: 5, max: 25, step: 5}"
      ],
      "metadata": {
        "id": "d4qZOamlwwym"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.optim import AdamW\n",
        "from torch.nn import CrossEntropyLoss, BCEWithLogitsLoss\n",
        "\n",
        "moe = MoEForEmotionAndTriggerClassification(gate_type = GATE_TYPE, expert_type = EXPERT_TYPE, num_experts = NUM_EXPERTS, k = TOP_K, num_classes = len(labels_to_ids))\n",
        "optimizer = AdamW(moe.parameters(), lr = LEARNING_RATE)\n",
        "\n",
        "emotion_loss_fn = CrossEntropyLoss()\n",
        "trigger_loss_fn = BCEWithLogitsLoss()"
      ],
      "metadata": {
        "id": "oJPBM0OaYWBf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "beb9ceb0-0cb4-446c-db55-190a06e50215"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "moe.to(device)"
      ],
      "metadata": {
        "id": "EiLsS9Y9YWjk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3042ba0f-b90e-419d-dc35-f59b26408a6f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "MoEForEmotionAndTriggerClassification(\n",
              "  (roberta): RobertaModel(\n",
              "    (embeddings): RobertaEmbeddings(\n",
              "      (word_embeddings): Embedding(50265, 768, padding_idx=1)\n",
              "      (position_embeddings): Embedding(514, 768, padding_idx=1)\n",
              "      (token_type_embeddings): Embedding(1, 768)\n",
              "      (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (encoder): RobertaEncoder(\n",
              "      (layer): ModuleList(\n",
              "        (0-11): 12 x RobertaLayer(\n",
              "          (attention): RobertaAttention(\n",
              "            (self): RobertaSdpaSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): RobertaSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): RobertaIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): RobertaOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (pooler): RobertaPooler(\n",
              "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "      (activation): Tanh()\n",
              "    )\n",
              "  )\n",
              "  (gating_network_emotion): Sequential(\n",
              "    (0): Linear(in_features=768, out_features=512, bias=True)\n",
              "    (1): ReLU()\n",
              "    (2): Linear(in_features=512, out_features=4, bias=True)\n",
              "  )\n",
              "  (gating_network_trigger): Sequential(\n",
              "    (0): Linear(in_features=768, out_features=512, bias=True)\n",
              "    (1): ReLU()\n",
              "    (2): Linear(in_features=512, out_features=4, bias=True)\n",
              "  )\n",
              "  (experts): ModuleList(\n",
              "    (0-3): 4 x LSTM(768, 768)\n",
              "  )\n",
              "  (emotion_classifier): Linear(in_features=768, out_features=7, bias=True)\n",
              "  (trigger_classifier): Linear(in_features=768, out_features=1, bias=True)\n",
              "  (dropout): Dropout(p=0.1, inplace=False)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def remove_padding(logits, labels, task):\n",
        "    mask = labels != -1\n",
        "\n",
        "    logits_flat = logits.view(-1, logits.size(-1)) if task == 'emotion' else logits.view(-1)\n",
        "    labels_flat = labels.view(-1)\n",
        "\n",
        "    logits = logits_flat[mask.view(-1)]\n",
        "    labels = labels_flat[mask.view(-1)]\n",
        "\n",
        "    return logits, labels"
      ],
      "metadata": {
        "id": "p8nFDwIIMzcz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate(model, val_loader):\n",
        "    model.eval()\n",
        "    val_loss, nb_steps = 0.0, 0\n",
        "    total_emotion_preds, correct_emotion_preds = 0, 0\n",
        "    total_trigger_preds, correct_trigger_preds = 0, 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for idx, batch in enumerate(val_loader):\n",
        "            input_ids = batch['input_ids'].to(device)\n",
        "            attention_mask = batch['attention_mask'].to(device)\n",
        "            emotion_labels = batch['emotion_labels'].to(device)\n",
        "            trigger_labels = batch['trigger_labels'].to(device)\n",
        "\n",
        "            emotion_logits, trigger_logits = model(input_ids, attention_mask)\n",
        "\n",
        "            # removing padding\n",
        "            emotion_logits, emotion_labels = remove_padding(emotion_logits, emotion_labels, 'emotion')\n",
        "            trigger_logits, trigger_labels = remove_padding(trigger_logits, trigger_labels, 'trigger')\n",
        "\n",
        "            # calculating loss\n",
        "            emotion_loss = emotion_loss_fn(emotion_logits, emotion_labels.long())\n",
        "            trigger_loss = trigger_loss_fn(trigger_logits, trigger_labels)\n",
        "\n",
        "            loss = emotion_loss + trigger_loss\n",
        "            val_loss += loss.item()\n",
        "\n",
        "            # calculating accuracy\n",
        "            emotion_preds = torch.argmax(emotion_logits, dim=-1)\n",
        "            trigger_preds = (torch.sigmoid(trigger_logits).squeeze(-1) > 0.5).long()\n",
        "\n",
        "            correct_emotion_preds += torch.sum(emotion_preds == emotion_labels).item()\n",
        "            correct_trigger_preds += torch.sum(trigger_preds == trigger_labels).item()\n",
        "\n",
        "            total_emotion_preds += emotion_labels.numel()\n",
        "            total_trigger_preds += trigger_labels.numel()\n",
        "\n",
        "            nb_steps += 1\n",
        "\n",
        "            if idx % 100 == 0:\n",
        "                loss_step = val_loss / nb_steps\n",
        "                print(f'      Validation loss per 100 training steps: {loss_step}')\n",
        "\n",
        "        avg_val_loss = val_loss / len(val_loader)\n",
        "        emotion_accuracy = correct_emotion_preds / total_emotion_preds\n",
        "        trigger_accuracy = correct_trigger_preds / total_trigger_preds\n",
        "        avg_val_accuracy = (emotion_accuracy + trigger_accuracy)/2\n",
        "\n",
        "    return avg_val_loss, avg_val_accuracy"
      ],
      "metadata": {
        "id": "7QS5wNkiYZt7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_and_validate(model, train_loader, val_loader, num_epochs = 3):\n",
        "    for epoch in range(num_epochs):\n",
        "        print(f\"Epoch [{epoch + 1}/{num_epochs}]\")\n",
        "        model.train()\n",
        "        train_loss, nb_steps = 0.0, 0\n",
        "        total_emotion_preds, correct_emotion_preds = 0, 0\n",
        "        total_trigger_preds, correct_trigger_preds = 0, 0\n",
        "\n",
        "        for idx, batch in enumerate(train_loader):\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            input_ids = batch['input_ids'].to(device)\n",
        "            attention_mask = batch['attention_mask'].to(device)\n",
        "            emotion_labels = batch['emotion_labels'].to(device)\n",
        "            trigger_labels = batch['trigger_labels'].to(device)\n",
        "\n",
        "            emotion_logits, trigger_logits = model(input_ids, attention_mask)\n",
        "\n",
        "            # removing padding\n",
        "            emotion_logits, emotion_labels = remove_padding(emotion_logits, emotion_labels, 'emotion')\n",
        "            trigger_logits, trigger_labels = remove_padding(trigger_logits, trigger_labels, 'trigger')\n",
        "\n",
        "            # calculating loss\n",
        "            emotion_loss = emotion_loss_fn(emotion_logits, emotion_labels.long())\n",
        "            trigger_loss = trigger_loss_fn(trigger_logits, trigger_labels)\n",
        "\n",
        "            loss = emotion_loss + trigger_loss\n",
        "            train_loss += loss.item()\n",
        "\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            # calculating accuracy\n",
        "            emotion_preds = torch.argmax(emotion_logits, dim=-1)\n",
        "            trigger_preds = (torch.sigmoid(trigger_logits).squeeze(-1) > 0.5).long()\n",
        "\n",
        "            correct_emotion_preds += torch.sum(emotion_preds == emotion_labels).item()\n",
        "            correct_trigger_preds += torch.sum(trigger_preds == trigger_labels).item()\n",
        "\n",
        "            total_emotion_preds += emotion_labels.numel()\n",
        "            total_trigger_preds += trigger_labels.numel()\n",
        "            nb_steps += 1\n",
        "\n",
        "            if idx % 100 == 0:\n",
        "                loss_step = train_loss / nb_steps\n",
        "                print(f'      Training loss per 100 training steps: {loss_step}')\n",
        "\n",
        "        avg_train_loss = train_loss / len(train_loader)\n",
        "        emotion_accuracy = correct_emotion_preds / total_emotion_preds\n",
        "        trigger_accuracy = correct_trigger_preds / total_trigger_preds\n",
        "        avg_train_accuracy = (emotion_accuracy + trigger_accuracy)/2\n",
        "\n",
        "        val_loss, val_accuracy = evaluate(model, val_loader)\n",
        "\n",
        "        print(f\"   Training Loss: {avg_train_loss:.3f}, Training Accuracy: {avg_train_accuracy:.3f}\")\n",
        "        print(f\"   Validation Loss: {val_loss:.3f}, Validation Accuracy: {val_accuracy:.3f}\\n\")"
      ],
      "metadata": {
        "id": "6Cq8fwysYaY_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_and_validate(moe, train_loader, val_loader, num_epochs = NUM_EPOCHS)"
      ],
      "metadata": {
        "id": "N_U28Fs-YcFL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5253cfa9-9562-48dd-b411-4f883bda5a4f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/5]\n",
            "      Training loss per 100 training steps: 2.6143882274627686\n",
            "      Training loss per 100 training steps: 2.0990418903898487\n",
            "      Validation loss per 100 training steps: 2.03092360496521\n",
            "   Training Loss: 2.048, Training Accuracy: 0.638\n",
            "   Validation Loss: 1.987, Validation Accuracy: 0.638\n",
            "\n",
            "Epoch [2/5]\n",
            "      Training loss per 100 training steps: 1.8464374542236328\n",
            "      Training loss per 100 training steps: 1.9986186192767454\n",
            "      Validation loss per 100 training steps: 1.9590860605239868\n",
            "   Training Loss: 1.968, Training Accuracy: 0.639\n",
            "   Validation Loss: 1.932, Validation Accuracy: 0.636\n",
            "\n",
            "Epoch [3/5]\n",
            "      Training loss per 100 training steps: 1.7594891786575317\n",
            "      Training loss per 100 training steps: 1.9105298117835923\n",
            "      Validation loss per 100 training steps: 1.8150749206542969\n",
            "   Training Loss: 1.852, Training Accuracy: 0.647\n",
            "   Validation Loss: 1.793, Validation Accuracy: 0.657\n",
            "\n",
            "Epoch [4/5]\n",
            "      Training loss per 100 training steps: 1.6628975868225098\n",
            "      Training loss per 100 training steps: 1.7238984462058191\n",
            "      Validation loss per 100 training steps: 1.558345079421997\n",
            "   Training Loss: 1.648, Training Accuracy: 0.686\n",
            "   Validation Loss: 1.513, Validation Accuracy: 0.714\n",
            "\n",
            "Epoch [5/5]\n",
            "      Training loss per 100 training steps: 1.3875391483306885\n",
            "      Training loss per 100 training steps: 1.4493757864036183\n",
            "      Validation loss per 100 training steps: 1.2814793586730957\n",
            "   Training Loss: 1.342, Training Accuracy: 0.747\n",
            "   Validation Loss: 1.216, Validation Accuracy: 0.776\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.save(moe.state_dict(), f'trained_models/{DATASET}/moe_model_{GATE_TYPE}_gate_{NUM_EXPERTS}_{EXPERT_TYPE}_experts_{TOP_K}_active_{LEARNING_RATE}_lr_{NUM_EPOCHS}_epochs.pth')"
      ],
      "metadata": {
        "id": "s7hH0xNVYdqL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
        "\n",
        "def get_metrics(model, data_loader, dev):\n",
        "    model.eval()\n",
        "\n",
        "    emotion_accuracy = 0.0\n",
        "    emotion_precision = 0.0\n",
        "    emotion_recall = 0.0\n",
        "    emotion_f1 = 0.0\n",
        "\n",
        "    trigger_accuracy = 0.0\n",
        "    trigger_precision = 0.0\n",
        "    trigger_recall = 0.0\n",
        "    trigger_f1 = 0.0\n",
        "\n",
        "    num_samples, nb_steps = 0, 0\n",
        "\n",
        "    for batch in data_loader:\n",
        "        input_ids = batch['input_ids'].to(dev)\n",
        "        attention_mask = batch['attention_mask'].to(dev)\n",
        "        emotion_labels = batch['emotion_labels'].to(dev)\n",
        "        trigger_labels = batch['trigger_labels'].to(dev)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            # Forward pass\n",
        "            emotion_logits, trigger_logits = model(input_ids, attention_mask)\n",
        "\n",
        "            # Compute predictions for emotions\n",
        "            emotion_logits, emotion_labels = remove_padding(emotion_logits, emotion_labels, 'emotion')\n",
        "\n",
        "            emotion_preds = torch.argmax(emotion_logits, dim = -1)\n",
        "\n",
        "            emotion_preds_flat = emotion_preds.cpu().numpy()\n",
        "            emotion_labels_flat = emotion_labels.cpu().numpy()\n",
        "\n",
        "            # Compute predictions for triggers\n",
        "            trigger_logits, trigger_labels = remove_padding(trigger_logits, trigger_labels, 'trigger')\n",
        "\n",
        "            trigger_preds = (torch.sigmoid(trigger_logits).squeeze(-1) > 0.5).long()\n",
        "\n",
        "            trigger_preds_flat = trigger_preds.cpu().numpy()\n",
        "            trigger_labels_flat = trigger_labels.cpu().numpy()\n",
        "\n",
        "            # Calculate metrics for emotion classification\n",
        "            accuracy = accuracy_score(emotion_labels_flat, emotion_preds_flat)\n",
        "\n",
        "            precision, recall, f1, _ = precision_recall_fscore_support(\n",
        "                emotion_labels_flat, emotion_preds_flat, average='weighted', zero_division = 0\n",
        "            )\n",
        "\n",
        "            emotion_accuracy += accuracy\n",
        "            emotion_precision += precision\n",
        "            emotion_recall += recall\n",
        "            emotion_f1 += f1\n",
        "\n",
        "            # Calculate metrics for trigger classification\n",
        "            accuracy = accuracy_score(trigger_labels_flat, trigger_preds_flat)\n",
        "\n",
        "            precision, recall, f1, _ = precision_recall_fscore_support(\n",
        "                trigger_labels_flat, trigger_preds_flat, average='weighted', zero_division = 0\n",
        "            )\n",
        "\n",
        "            trigger_accuracy += accuracy\n",
        "            trigger_precision += precision\n",
        "            trigger_recall += recall\n",
        "            trigger_f1 += f1\n",
        "\n",
        "            nb_steps += 1\n",
        "\n",
        "    # Calculate average metrics\n",
        "    avg_emotion_accuracy = emotion_accuracy / nb_steps\n",
        "    avg_emotion_precision = emotion_precision / nb_steps\n",
        "    avg_emotion_recall = emotion_recall / nb_steps\n",
        "    avg_emotion_f1 = emotion_f1 / nb_steps\n",
        "\n",
        "    avg_trigger_accuracy = trigger_accuracy / nb_steps\n",
        "    avg_trigger_precision = trigger_precision / nb_steps\n",
        "    avg_trigger_recall = trigger_recall / nb_steps\n",
        "    avg_trigger_f1 = trigger_f1 / nb_steps\n",
        "\n",
        "    return (avg_emotion_accuracy, avg_emotion_precision, avg_emotion_recall, avg_emotion_f1,\n",
        "            avg_trigger_accuracy, avg_trigger_precision, avg_trigger_recall, avg_trigger_f1)"
      ],
      "metadata": {
        "id": "CtoS1ploVP0i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "avg_emotion_accuracy, avg_emotion_precision, avg_emotion_recall, avg_emotion_f1, avg_trigger_accuracy, avg_trigger_precision, avg_trigger_recall, avg_trigger_f1 = get_metrics(moe, test_loader, device)\n",
        "\n",
        "# Output results\n",
        "print(\"Emotion classification:\")\n",
        "print(f\"   Accuracy: {avg_emotion_accuracy:.3f}\")\n",
        "print(f\"   Precision: {avg_emotion_precision:.3f}\")\n",
        "print(f\"   Recall: {avg_emotion_recall:.3f}\")\n",
        "print(f\"   F1-score: {avg_emotion_f1:.3f}\")\n",
        "\n",
        "print(\"\\n Trigger classification:\")\n",
        "print(f\"   Accuracy: {avg_trigger_accuracy:.3f}\")\n",
        "print(f\"   Precision: {avg_trigger_precision:.3f}\")\n",
        "print(f\"   Recall: {avg_trigger_recall:.3f}\")\n",
        "print(f\"   F1-score: {avg_trigger_f1:.3f}\")"
      ],
      "metadata": {
        "id": "_8mXtjNtVSir"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Load and test trained model"
      ],
      "metadata": {
        "id": "6pKrBtyNVUes"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "moe_loaded = MoEForEmotionAndTriggerClassification(gate_type = GATE_TYPE, expert_type = EXPERT_TYPE, num_experts = NUM_EXPERTS, k = TOP_K, num_classes = len(labels_to_ids))\n",
        "moe_loaded.load_state_dict(torch.load(f'trained_models/{DATASET}/moe_model_{GATE_TYPE}_gate_{NUM_EXPERTS}_{EXPERT_TYPE}_experts_{TOP_K}_active_{LEARNING_RATE}_lr_{NUM_EPOCHS}_epochs.pth', map_location=torch.device('cpu')))"
      ],
      "metadata": {
        "id": "Y8SZCnMSYfQU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f5d5b6c4-1870-4708-cf88-d6dc30bb9588"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "<ipython-input-51-b016ff191b10>:2: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  moe_loaded.load_state_dict(torch.load(f'trained_models/moe_model_{GATE_TYPE}_gate_{NUM_EXPERTS}_{EXPERT_TYPE}_experts_{TOP_K}_active_{LEARNING_RATE}_lr_{NUM_EPOCHS}_epochs.pth', map_location=torch.device(\"cpu\")))\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {},
          "execution_count": 51
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "avg_emotion_accuracy, avg_emotion_precision, avg_emotion_recall, avg_emotion_f1, avg_trigger_accuracy, avg_trigger_precision, avg_trigger_recall, avg_trigger_f1 = get_metrics(moe_loaded, test_loader, 'cpu')\n",
        "\n",
        "# Output results\n",
        "print(\"Emotion classification:\")\n",
        "print(f\"   Accuracy: {avg_emotion_accuracy:.3f}\")\n",
        "print(f\"   Precision: {avg_emotion_precision:.3f}\")\n",
        "print(f\"   Recall: {avg_emotion_recall:.3f}\")\n",
        "print(f\"   F1-score: {avg_emotion_f1:.3f}\")\n",
        "\n",
        "print(\"\\n Trigger classification:\")\n",
        "print(f\"   Accuracy: {avg_trigger_accuracy:.3f}\")\n",
        "print(f\"   Precision: {avg_trigger_precision:.3f}\")\n",
        "print(f\"   Recall: {avg_trigger_recall:.3f}\")\n",
        "print(f\"   F1-score: {avg_trigger_f1:.3f}\")"
      ],
      "metadata": {
        "id": "VAHFusyhYmhV"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}