{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4YS-HHlMxdzr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8fa11d28-d935-41a6-c469-fa0fef5abb66"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd drive/MyDrive/Colab \\Notebooks"
      ],
      "metadata": {
        "id": "hMHoUEE8xvX0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3aeac77b-c7fb-429b-ed5e-557b4ec38c36"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Errno 2] No such file or directory: 'drive/MyDrive'\n",
            "/content/drive/MyDrive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from transformers import RobertaTokenizer, RobertaModel\n",
        "import pandas as pd\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "M6Woq9BFW18k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch import cuda\n",
        "device = 'cuda' if cuda.is_available() else 'cpu'\n",
        "\n",
        "print(device)"
      ],
      "metadata": {
        "id": "4QJ_vvYdW2YT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b507b53a-cd88-4b4d-ba64-bfd162e207d9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data preparation"
      ],
      "metadata": {
        "id": "DWH--rE_zW7H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "DATASET = 'MELD'    # @param ['MELD', 'MaSaC']\n",
        "MAX_LENGTH = 128    # @param [96, 128, 256] {type: 'raw'}\n",
        "BATCH_SIZE = 16    # @param [8, 16, 32] {type: 'raw'}"
      ],
      "metadata": {
        "id": "L86nTmkQXOp-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_df = pd.read_json('data/EDiReF_train_data/MELD_train_efr.json')\n",
        "train_df[\"triggers\"] = train_df[\"triggers\"].apply(lambda lst: [np.nan if x is None else x for x in lst])\n",
        "train_df = train_df[train_df[\"triggers\"].apply(lambda lst: not any(pd.isna(x) for x in lst))]\n",
        "\n",
        "flattened_emotions = [sent for conv in train_df['emotions'] for sent in conv]\n",
        "unique_emotions = set(flattened_emotions)\n",
        "\n",
        "labels_to_ids = {k: v for v, k in enumerate(unique_emotions)}\n",
        "ids_to_labels = {v: k for v, k in enumerate(unique_emotions)}\n",
        "\n",
        "train_conversations = list(train_df['utterances'])\n",
        "train_emotions = [[labels_to_ids[emotion] for emotion in conv] for conv in list(train_df['emotions'])]\n",
        "train_triggers = list(train_df['triggers'])"
      ],
      "metadata": {
        "id": "C7KmZUkRW3tz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "val_df = pd.read_json('data/EDiReF_val_data/MELD_val_efr.json')\n",
        "val_df[\"triggers\"] = val_df[\"triggers\"].apply(lambda lst: [np.nan if x is None else x for x in lst])\n",
        "val_df = val_df[val_df[\"triggers\"].apply(lambda lst: not any(pd.isna(x) for x in lst))]\n",
        "\n",
        "val_conversations = list(val_df['utterances'])\n",
        "val_emotions = [[labels_to_ids[emotion] for emotion in conv] for conv in list(val_df['emotions'])]\n",
        "val_triggers = list(val_df['triggers'])"
      ],
      "metadata": {
        "id": "CRuTkn5tW497"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "conversations = train_conversations + val_conversations\n",
        "emotions = train_emotions + val_emotions\n",
        "triggers = train_triggers + val_triggers"
      ],
      "metadata": {
        "id": "Yk639vChPn6a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "def train_val_test_split(X, y1, y2, val_size = 0.2, test_size = 0.2, random_state = None):\n",
        "    X_train_val, X_test, y1_train_val, y1_test, y2_train_val, y2_test = train_test_split(\n",
        "        X, y1, y2, test_size=test_size, random_state=random_state\n",
        "    )\n",
        "\n",
        "    val_relative_size = val_size / (1 - test_size)\n",
        "\n",
        "    X_train, X_val, y1_train, y1_val, y2_train, y2_val = train_test_split(\n",
        "        X_train_val, y1_train_val, y2_train_val, test_size=val_relative_size, random_state=random_state\n",
        "    )\n",
        "\n",
        "    return (X_train, X_val, X_test, y1_train, y1_val, y1_test, y2_train, y2_val, y2_test)"
      ],
      "metadata": {
        "id": "mi-y0zznb-hG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, X_val, X_test, y1_train, y1_val, y1_test, y2_train, y2_val, y2_test = train_val_test_split(\n",
        "    conversations, emotions, triggers, test_size=0.15, val_size=0.15, random_state=2024\n",
        "    )"
      ],
      "metadata": {
        "id": "arnKxaiEeoLD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = RobertaTokenizer.from_pretrained('roberta-base')"
      ],
      "metadata": {
        "id": "C5430Zq1W-dj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def tokenize_conversation(conversations, max_length = 128):\n",
        "    input_ids = []\n",
        "    attention_masks = []\n",
        "\n",
        "    for conversation in conversations:\n",
        "        dialogue = \" [SEP] \".join(conversation)\n",
        "        encoded = tokenizer(\n",
        "            dialogue,\n",
        "            truncation = True,\n",
        "            padding = 'max_length',\n",
        "            max_length = max_length,\n",
        "            return_tensors = \"pt\"\n",
        "        )\n",
        "        input_ids.append(encoded[\"input_ids\"].squeeze(0))\n",
        "        attention_masks.append(encoded[\"attention_mask\"].squeeze(0))\n",
        "\n",
        "    return input_ids, attention_masks"
      ],
      "metadata": {
        "id": "ng_4Q4k1XJvc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def pad_labels(labels, max_length = 128):\n",
        "    padded_labels = []\n",
        "    for label_set in labels:\n",
        "        label_tensor = torch.tensor(label_set, dtype = torch.float)\n",
        "        # Pad with -1 to ignore padding tokens in the loss function\n",
        "        padded_tensor = torch.cat(\n",
        "            [label_tensor, torch.full((max_length - len(label_set),), -1)]\n",
        "        )\n",
        "        padded_labels.append(padded_tensor)\n",
        "    return padded_labels"
      ],
      "metadata": {
        "id": "kUdbsdw_XMrD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ConversationDataset(Dataset):\n",
        "    def __init__(self, input_ids, attention_masks, emotion_labels, trigger_labels):\n",
        "        self.input_ids = input_ids\n",
        "        self.attention_masks = attention_masks\n",
        "        self.emotion_labels = emotion_labels\n",
        "        self.trigger_labels = trigger_labels\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.input_ids)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return {\n",
        "            \"input_ids\": self.input_ids[idx],\n",
        "            \"attention_mask\": self.attention_masks[idx],\n",
        "            \"emotion_labels\": self.emotion_labels[idx],\n",
        "            \"trigger_labels\": self.trigger_labels[idx],\n",
        "        }"
      ],
      "metadata": {
        "id": "8pcFsfoZXNBD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_input_ids, train_attention_masks = tokenize_conversation(X_train, max_length = MAX_LENGTH)\n",
        "\n",
        "train_emotion_labels = pad_labels(y1_train, max_length = MAX_LENGTH)\n",
        "train_trigger_labels = pad_labels(y2_train, max_length = MAX_LENGTH)\n",
        "\n",
        "train_dataset = ConversationDataset(train_input_ids, train_attention_masks, train_emotion_labels, train_trigger_labels)"
      ],
      "metadata": {
        "id": "0vvyi2PbXQaL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "val_input_ids, val_attention_masks = tokenize_conversation(X_val, max_length = MAX_LENGTH)\n",
        "\n",
        "val_emotion_labels = pad_labels(y1_val, max_length = MAX_LENGTH)\n",
        "val_trigger_labels = pad_labels(y2_val, max_length = MAX_LENGTH)\n",
        "\n",
        "val_dataset = ConversationDataset(val_input_ids, val_attention_masks, val_emotion_labels, val_trigger_labels)"
      ],
      "metadata": {
        "id": "YqCBE1sMXSX9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_input_ids, test_attention_masks = tokenize_conversation(X_test, max_length = MAX_LENGTH)\n",
        "\n",
        "test_emotion_labels = pad_labels(y1_test, max_length = MAX_LENGTH)\n",
        "test_trigger_labels = pad_labels(y2_test, max_length = MAX_LENGTH)\n",
        "\n",
        "test_dataset = ConversationDataset(test_input_ids, test_attention_masks, test_emotion_labels, test_trigger_labels)"
      ],
      "metadata": {
        "id": "23bPP2F2P476"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_loader = DataLoader(train_dataset, batch_size = BATCH_SIZE, shuffle = False)\n",
        "val_loader = DataLoader(val_dataset, batch_size = BATCH_SIZE, shuffle = False)\n",
        "test_loader = DataLoader(test_dataset, batch_size = BATCH_SIZE, shuffle = False)"
      ],
      "metadata": {
        "id": "UEtU-77cXT8b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model configuration"
      ],
      "metadata": {
        "id": "X5MO3g0VztW_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "GATE_TYPE = 'linear'  # @param ['linear', 'mlp']\n",
        "EXPERT_TYPE = 'linear' # @param ['linear', 'mlp', 'rnn']\n",
        "NUM_EXPERTS = 2 # @param {type: 'slider', min: 1, max: 8, step: 1}\n",
        "TOP_K = 2 # @param {type: 'slider', min: 1, max: 8, step: 1}"
      ],
      "metadata": {
        "id": "kueFTpHSXViM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "assert TOP_K <= NUM_EXPERTS, \"Select different values for TOP_K and NUM_EXPERTS!\""
      ],
      "metadata": {
        "id": "12upnrxiXXbM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MoEForEmotionAndTriggerClassification(nn.Module):\n",
        "    def __init__(self, gate_type = 'linear', expert_type = 'linear', num_experts, k, num_classes):\n",
        "        super(MoEForEmotionAndTriggerClassification, self).__init__()\n",
        "\n",
        "        self.roberta = RobertaModel.from_pretrained('roberta-base')\n",
        "        for param in self.roberta.parameters():\n",
        "            param.requires_grad = True  # Set to True if you want to fine-tune RoBERTa\n",
        "        hidden_size = self.roberta.config.hidden_size\n",
        "\n",
        "        gate_setup = {\n",
        "            'linear': nn.Linear(hidden_size, num_experts),\n",
        "            'mlp': nn.Sequential(nn.Linear(hidden_size, 512), nn.ReLU(), nn.Linear(512, num_experts)),\n",
        "        }\n",
        "\n",
        "        expert_setup = {\n",
        "            'linear': nn.Linear(hidden_size, hidden_size),\n",
        "            'mlp': nn.Sequential(nn.Linear(hidden_size, 512), nn.ReLU(), nn.Linear(512, hidden_size)),\n",
        "            'rnn': nn.LSTM(hidden_size, hidden_size),\n",
        "        }\n",
        "\n",
        "        self.gating_network = gate_setup[GATE_TYPE]\n",
        "        self.experts = nn.ModuleList([expert_setup[EXPERT_TYPE] for _ in range(num_experts)])\n",
        "\n",
        "        self.emotion_classifier = nn.Linear(hidden_size, num_classes)\n",
        "        self.trigger_classifier = nn.Linear(hidden_size, 1)\n",
        "\n",
        "        self.k = k\n",
        "        self.dropout = nn.Dropout(p = 0.1)\n",
        "\n",
        "    def forward(self, input_ids, attention_mask):\n",
        "        roberta_outputs = self.roberta(input_ids = input_ids, attention_mask = attention_mask)\n",
        "        embeddings = roberta_outputs.last_hidden_state  # (batch_size, seq_len, hidden_size)\n",
        "        pooled_embeddings = embeddings.mean(dim = 1)    # (batch_size, hidden_size)\n",
        "        pooled_embeddings = self.dropout(pooled_embeddings)\n",
        "\n",
        "        # expert weights\n",
        "        expert_weights = self.gating_network(pooled_embeddings) # (batch_size, num_experts)\n",
        "        expert_weights = torch.softmax(expert_weights, dim = -1)\n",
        "\n",
        "        # aggregate expert outputs\n",
        "        combined_output = self._compute_expert_output(embeddings, expert_weights)\n",
        "        combined_output = self.dropout(combined_output)\n",
        "\n",
        "        emotion_logits = self.emotion_classifier(combined_output)   # (batch_size, seq_len, num_classes)\n",
        "        trigger_logits = self.trigger_classifier(combined_output).squeeze(-1)   # (batch_size, seq_len)\n",
        "\n",
        "        return emotion_logits, trigger_logits\n",
        "\n",
        "    def _compute_expert_output(self, embeddings, expert_weights):\n",
        "        batch_size, num_experts = expert_weights.size()\n",
        "        combined_output = torch.zeros_like(embeddings)\n",
        "\n",
        "        # top-k experts only are activated\n",
        "        topk_weights, topk_indices = torch.topk(expert_weights, self.k, dim = -1)\n",
        "\n",
        "        for i in range(self.k):\n",
        "            expert_idx = topk_indices[:, i]\n",
        "            weight = topk_weights[:, i].unsqueeze(-1).unsqueeze(-1)\n",
        "\n",
        "            expert_outputs = []\n",
        "            for j in range(expert_idx.size(0)):\n",
        "                expert = self.experts[expert_idx[j]]\n",
        "\n",
        "                if isinstance(expert, nn.LSTM):\n",
        "                    embedding_input = embeddings[j].unsqueeze(0)\n",
        "                    output, _ = expert(embedding_input)\n",
        "                    expert_outputs.append(output.squeeze(0))\n",
        "\n",
        "                elif isinstance(expert, nn.Linear) or isinstance(expert, nn.Sequential):\n",
        "                    output = expert(embeddings[j])\n",
        "                    expert_outputs.append(output)\n",
        "\n",
        "            expert_outputs = torch.stack(expert_outputs)\n",
        "            combined_output += weight * expert_outputs\n",
        "\n",
        "        return combined_output"
      ],
      "metadata": {
        "id": "umr0EC9_XuzD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training parameters"
      ],
      "metadata": {
        "id": "Fpyq3mUpz6D2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "LEARNING_RATE = 0.00002  # @param {type: 'slider', min: 1E-5, max: 5E-5, step: 1E-5}\n",
        "NUM_EPOCHS = 10  # @param {type: 'slider', min: 5, max: 25, step: 5}"
      ],
      "metadata": {
        "id": "eO2uRZl2zxvO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.optim import AdamW\n",
        "from torch.nn import CrossEntropyLoss, BCEWithLogitsLoss\n",
        "\n",
        "moe = MoEForEmotionAndTriggerClassification(gate_type = GATE_TYPE, expert_type = EXPERT_TYPE, num_experts = NUM_EXPERTS, k = TOP_K, num_classes = len(labels_to_ids))\n",
        "optimizer = AdamW(moe.parameters(), lr = LEARNING_RATE)\n",
        "\n",
        "emotion_loss_fn = CrossEntropyLoss()\n",
        "trigger_loss_fn = BCEWithLogitsLoss()"
      ],
      "metadata": {
        "id": "oJPBM0OaYWBf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3aa711b5-d970-4f4d-f7d4-584ddfdbd0f7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "moe.to(device)"
      ],
      "metadata": {
        "id": "EiLsS9Y9YWjk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "97b86eed-c831-4faf-ecef-88093ae47806"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "MoEForEmotionAndTriggerClassification(\n",
              "  (roberta): RobertaModel(\n",
              "    (embeddings): RobertaEmbeddings(\n",
              "      (word_embeddings): Embedding(50265, 768, padding_idx=1)\n",
              "      (position_embeddings): Embedding(514, 768, padding_idx=1)\n",
              "      (token_type_embeddings): Embedding(1, 768)\n",
              "      (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (encoder): RobertaEncoder(\n",
              "      (layer): ModuleList(\n",
              "        (0-11): 12 x RobertaLayer(\n",
              "          (attention): RobertaAttention(\n",
              "            (self): RobertaSdpaSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): RobertaSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): RobertaIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): RobertaOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (pooler): RobertaPooler(\n",
              "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "      (activation): Tanh()\n",
              "    )\n",
              "  )\n",
              "  (gating_network): Linear(in_features=768, out_features=2, bias=True)\n",
              "  (experts): ModuleList(\n",
              "    (0-1): 2 x Linear(in_features=768, out_features=768, bias=True)\n",
              "  )\n",
              "  (emotion_classifier): Linear(in_features=768, out_features=7, bias=True)\n",
              "  (trigger_classifier): Linear(in_features=768, out_features=1, bias=True)\n",
              "  (dropout): Dropout(p=0.1, inplace=False)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 81
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def remove_padding(logits, labels, task):\n",
        "    mask = labels != -1\n",
        "\n",
        "    logits_flat = logits.view(-1, logits.size(-1)) if task == 'emotion' else logits.view(-1)\n",
        "    labels_flat = labels.view(-1)\n",
        "\n",
        "    logits = logits_flat[mask.view(-1)]\n",
        "    labels = labels_flat[mask.view(-1)]\n",
        "\n",
        "    return logits, labels"
      ],
      "metadata": {
        "id": "44x3zphNQWrq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate(model, val_loader):\n",
        "    model.eval()\n",
        "    val_loss, nb_steps = 0.0, 0\n",
        "    total_emotion_preds, correct_emotion_preds = 0, 0\n",
        "    total_trigger_preds, correct_trigger_preds = 0, 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for idx, batch in enumerate(val_loader):\n",
        "            input_ids = batch['input_ids'].to(device)\n",
        "            attention_mask = batch['attention_mask'].to(device)\n",
        "            emotion_labels = batch['emotion_labels'].to(device)\n",
        "            trigger_labels = batch['trigger_labels'].to(device)\n",
        "\n",
        "            emotion_logits, trigger_logits = model(input_ids, attention_mask)\n",
        "\n",
        "            # removing padding\n",
        "            emotion_logits, emotion_labels = remove_padding(emotion_logits, emotion_labels, 'emotion')\n",
        "            trigger_logits, trigger_labels = remove_padding(trigger_logits, trigger_labels, 'trigger')\n",
        "\n",
        "            # calculating loss\n",
        "            emotion_loss = emotion_loss_fn(emotion_logits, emotion_labels.long())\n",
        "            trigger_loss = trigger_loss_fn(trigger_logits, trigger_labels)\n",
        "\n",
        "            loss = emotion_loss + trigger_loss\n",
        "            val_loss += loss.item()\n",
        "\n",
        "            # calculating accuracy\n",
        "            emotion_preds = torch.argmax(emotion_logits, dim=-1)\n",
        "            trigger_preds = (torch.sigmoid(trigger_logits).squeeze(-1) > 0.5).long()\n",
        "\n",
        "            correct_emotion_preds += torch.sum(emotion_preds == emotion_labels).item()\n",
        "            correct_trigger_preds += torch.sum(trigger_preds == trigger_labels).item()\n",
        "\n",
        "            total_emotion_preds += emotion_labels.numel()\n",
        "            total_trigger_preds += trigger_labels.numel()\n",
        "\n",
        "            nb_steps += 1\n",
        "\n",
        "            if idx % 100 == 0:\n",
        "                loss_step = val_loss / nb_steps\n",
        "                print(f'      Validation loss per 100 training steps: {loss_step}')\n",
        "\n",
        "        avg_val_loss = val_loss / len(val_loader)\n",
        "        emotion_accuracy = correct_emotion_preds / total_emotion_preds\n",
        "        trigger_accuracy = correct_trigger_preds / total_trigger_preds\n",
        "        avg_val_accuracy = (emotion_accuracy + trigger_accuracy)/2\n",
        "\n",
        "    return avg_val_loss, avg_val_accuracy"
      ],
      "metadata": {
        "id": "7QS5wNkiYZt7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_and_validate(model, train_loader, val_loader, num_epochs = 3):\n",
        "    for epoch in range(num_epochs):\n",
        "        print(f\"Epoch [{epoch + 1}/{num_epochs}]\")\n",
        "        model.train()\n",
        "        train_loss, nb_steps = 0.0, 0\n",
        "        total_emotion_preds, correct_emotion_preds = 0, 0\n",
        "        total_trigger_preds, correct_trigger_preds = 0, 0\n",
        "\n",
        "        for idx, batch in enumerate(train_loader):\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            input_ids = batch['input_ids'].to(device)\n",
        "            attention_mask = batch['attention_mask'].to(device)\n",
        "            emotion_labels = batch['emotion_labels'].to(device)\n",
        "            trigger_labels = batch['trigger_labels'].to(device)\n",
        "\n",
        "            emotion_logits, trigger_logits = model(input_ids, attention_mask)\n",
        "\n",
        "            # removing padding\n",
        "            emotion_logits, emotion_labels = remove_padding(emotion_logits, emotion_labels, 'emotion')\n",
        "            trigger_logits, trigger_labels = remove_padding(trigger_logits, trigger_labels, 'trigger')\n",
        "\n",
        "            # calculating loss\n",
        "            emotion_loss = emotion_loss_fn(emotion_logits, emotion_labels.long())\n",
        "            trigger_loss = trigger_loss_fn(trigger_logits, trigger_labels)\n",
        "\n",
        "            loss = emotion_loss + trigger_loss\n",
        "            train_loss += loss.item()\n",
        "\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            # calculating accuracy\n",
        "            emotion_preds = torch.argmax(emotion_logits, dim=-1)\n",
        "            trigger_preds = (torch.sigmoid(trigger_logits).squeeze(-1) > 0.5).long()\n",
        "\n",
        "            correct_emotion_preds += torch.sum(emotion_preds == emotion_labels).item()\n",
        "            correct_trigger_preds += torch.sum(trigger_preds == trigger_labels).item()\n",
        "\n",
        "            total_emotion_preds += emotion_labels.numel()\n",
        "            total_trigger_preds += trigger_labels.numel()\n",
        "            nb_steps += 1\n",
        "\n",
        "            if idx % 100 == 0:\n",
        "                loss_step = train_loss / nb_steps\n",
        "                print(f'      Training loss per 100 training steps: {loss_step}')\n",
        "\n",
        "        avg_train_loss = train_loss / len(train_loader)\n",
        "        emotion_accuracy = correct_emotion_preds / total_emotion_preds\n",
        "        trigger_accuracy = correct_trigger_preds / total_trigger_preds\n",
        "        avg_train_accuracy = (emotion_accuracy + trigger_accuracy)/2\n",
        "\n",
        "        val_loss, val_accuracy = evaluate(model, val_loader)\n",
        "\n",
        "        print(f\"   Training Loss: {avg_train_loss:.3f}, Training Accuracy: {avg_train_accuracy:.3f}\")\n",
        "        print(f\"   Validation Loss: {val_loss:.3f}, Validation Accuracy: {val_accuracy:.3f}\\n\")"
      ],
      "metadata": {
        "id": "6Cq8fwysYaY_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_and_validate(moe, train_loader, val_loader, num_epochs = NUM_EPOCHS)"
      ],
      "metadata": {
        "id": "N_U28Fs-YcFL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "73475786-08d0-4eef-cf54-9ecf13dd55ed"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/10]\n",
            "      Training loss per 100 training steps: 2.613065481185913\n",
            "      Training loss per 100 training steps: 2.0700942041850325\n",
            "      Validation loss per 100 training steps: 1.8622701168060303\n",
            "   Training Loss: 1.990, Training Accuracy: 0.639\n",
            "   Validation Loss: 1.833, Validation Accuracy: 0.656\n",
            "\n",
            "Epoch [2/10]\n",
            "      Training loss per 100 training steps: 1.7236175537109375\n",
            "      Training loss per 100 training steps: 1.8152532483091448\n",
            "      Validation loss per 100 training steps: 1.5877764225006104\n",
            "   Training Loss: 1.736, Training Accuracy: 0.672\n",
            "   Validation Loss: 1.592, Validation Accuracy: 0.694\n",
            "\n",
            "Epoch [3/10]\n",
            "      Training loss per 100 training steps: 1.4812678098678589\n",
            "      Training loss per 100 training steps: 1.540615979987796\n",
            "      Validation loss per 100 training steps: 1.3325444459915161\n",
            "   Training Loss: 1.455, Training Accuracy: 0.722\n",
            "   Validation Loss: 1.357, Validation Accuracy: 0.745\n",
            "\n",
            "Epoch [4/10]\n",
            "      Training loss per 100 training steps: 1.1693326234817505\n",
            "      Training loss per 100 training steps: 1.2722825095205024\n",
            "      Validation loss per 100 training steps: 1.0827455520629883\n",
            "   Training Loss: 1.193, Training Accuracy: 0.772\n",
            "   Validation Loss: 1.080, Validation Accuracy: 0.801\n",
            "\n",
            "Epoch [5/10]\n",
            "      Training loss per 100 training steps: 1.0262174606323242\n",
            "      Training loss per 100 training steps: 1.0093843830694067\n",
            "      Validation loss per 100 training steps: 0.9649808406829834\n",
            "   Training Loss: 0.947, Training Accuracy: 0.821\n",
            "   Validation Loss: 0.956, Validation Accuracy: 0.822\n",
            "\n",
            "Epoch [6/10]\n",
            "      Training loss per 100 training steps: 0.7407810688018799\n",
            "      Training loss per 100 training steps: 0.8163886359422514\n",
            "      Validation loss per 100 training steps: 0.8785380125045776\n",
            "   Training Loss: 0.774, Training Accuracy: 0.852\n",
            "   Validation Loss: 0.842, Validation Accuracy: 0.839\n",
            "\n",
            "Epoch [7/10]\n",
            "      Training loss per 100 training steps: 0.5659466981887817\n",
            "      Training loss per 100 training steps: 0.6682378661514509\n",
            "      Validation loss per 100 training steps: 0.7787647843360901\n",
            "   Training Loss: 0.636, Training Accuracy: 0.875\n",
            "   Validation Loss: 0.795, Validation Accuracy: 0.850\n",
            "\n",
            "Epoch [8/10]\n",
            "      Training loss per 100 training steps: 0.5437910556793213\n",
            "      Training loss per 100 training steps: 0.6038930900026076\n",
            "      Validation loss per 100 training steps: 0.7927462458610535\n",
            "   Training Loss: 0.571, Training Accuracy: 0.887\n",
            "   Validation Loss: 0.803, Validation Accuracy: 0.851\n",
            "\n",
            "Epoch [9/10]\n",
            "      Training loss per 100 training steps: 0.45502352714538574\n",
            "      Training loss per 100 training steps: 0.5012982738490152\n",
            "      Validation loss per 100 training steps: 0.9079784154891968\n",
            "   Training Loss: 0.488, Training Accuracy: 0.901\n",
            "   Validation Loss: 0.762, Validation Accuracy: 0.862\n",
            "\n",
            "Epoch [10/10]\n",
            "      Training loss per 100 training steps: 0.33166277408599854\n",
            "      Training loss per 100 training steps: 0.4505788105549199\n",
            "      Validation loss per 100 training steps: 0.8649852871894836\n",
            "   Training Loss: 0.435, Training Accuracy: 0.910\n",
            "   Validation Loss: 0.762, Validation Accuracy: 0.867\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.save(moe.state_dict(), f'trained_models/{DATASET}/moe_model_{GATE_TYPE}_gate_{NUM_EXPERTS}_{EXPERT_TYPE}_experts_{TOP_K}_active_{LEARNING_RATE}_lr_{NUM_EPOCHS}_epochs.pth')"
      ],
      "metadata": {
        "id": "s7hH0xNVYdqL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
        "\n",
        "def get_metrics(model, data_loader, dev):\n",
        "    model.eval()\n",
        "\n",
        "    emotion_accuracy = 0.0\n",
        "    emotion_precision = 0.0\n",
        "    emotion_recall = 0.0\n",
        "    emotion_f1 = 0.0\n",
        "\n",
        "    trigger_accuracy = 0.0\n",
        "    trigger_precision = 0.0\n",
        "    trigger_recall = 0.0\n",
        "    trigger_f1 = 0.0\n",
        "\n",
        "    num_samples, nb_steps = 0, 0\n",
        "\n",
        "    for batch in data_loader:\n",
        "        input_ids = batch['input_ids'].to(dev)\n",
        "        attention_mask = batch['attention_mask'].to(dev)\n",
        "        emotion_labels = batch['emotion_labels'].to(dev)\n",
        "        trigger_labels = batch['trigger_labels'].to(dev)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            # Forward pass\n",
        "            emotion_logits, trigger_logits = model(input_ids, attention_mask)\n",
        "\n",
        "            # Compute predictions for emotions\n",
        "            emotion_logits, emotion_labels = remove_padding(emotion_logits, emotion_labels, 'emotion')\n",
        "\n",
        "            emotion_preds = torch.argmax(emotion_logits, dim = -1)\n",
        "\n",
        "            emotion_preds_flat = emotion_preds.cpu().numpy()\n",
        "            emotion_labels_flat = emotion_labels.cpu().numpy()\n",
        "\n",
        "            # Compute predictions for triggers\n",
        "            trigger_logits, trigger_labels = remove_padding(trigger_logits, trigger_labels, 'trigger')\n",
        "\n",
        "            trigger_preds = (torch.sigmoid(trigger_logits).squeeze(-1) > 0.5).long()\n",
        "\n",
        "            trigger_preds_flat = trigger_preds.cpu().numpy()\n",
        "            trigger_labels_flat = trigger_labels.cpu().numpy()\n",
        "\n",
        "            # Calculate metrics for emotion classification\n",
        "            accuracy = accuracy_score(emotion_labels_flat, emotion_preds_flat)\n",
        "\n",
        "            precision, recall, f1, _ = precision_recall_fscore_support(\n",
        "                emotion_labels_flat, emotion_preds_flat, average='weighted', zero_division = 0\n",
        "            )\n",
        "\n",
        "            emotion_accuracy += accuracy\n",
        "            emotion_precision += precision\n",
        "            emotion_recall += recall\n",
        "            emotion_f1 += f1\n",
        "\n",
        "            # Calculate metrics for trigger classification\n",
        "            accuracy = accuracy_score(trigger_labels_flat, trigger_preds_flat)\n",
        "\n",
        "            precision, recall, f1, _ = precision_recall_fscore_support(\n",
        "                trigger_labels_flat, trigger_preds_flat, average='weighted', zero_division = 0\n",
        "            )\n",
        "\n",
        "            trigger_accuracy += accuracy\n",
        "            trigger_precision += precision\n",
        "            trigger_recall += recall\n",
        "            trigger_f1 += f1\n",
        "\n",
        "            nb_steps += 1\n",
        "\n",
        "    # Calculate average metrics\n",
        "    avg_emotion_accuracy = emotion_accuracy / nb_steps\n",
        "    avg_emotion_precision = emotion_precision / nb_steps\n",
        "    avg_emotion_recall = emotion_recall / nb_steps\n",
        "    avg_emotion_f1 = emotion_f1 / nb_steps\n",
        "\n",
        "    avg_trigger_accuracy = trigger_accuracy / nb_steps\n",
        "    avg_trigger_precision = trigger_precision / nb_steps\n",
        "    avg_trigger_recall = trigger_recall / nb_steps\n",
        "    avg_trigger_f1 = trigger_f1 / nb_steps\n",
        "\n",
        "    return (avg_emotion_accuracy, avg_emotion_precision, avg_emotion_recall, avg_emotion_f1,\n",
        "            avg_trigger_accuracy, avg_trigger_precision, avg_trigger_recall, avg_trigger_f1)"
      ],
      "metadata": {
        "id": "2To3TjUdUEsR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "avg_emotion_accuracy, avg_emotion_precision, avg_emotion_recall, avg_emotion_f1, avg_trigger_accuracy, avg_trigger_precision, avg_trigger_recall, avg_trigger_f1 = get_metrics(moe, test_loader, device)\n",
        "\n",
        "# Output results\n",
        "print(\"Emotion classification:\")\n",
        "print(f\"   Accuracy: {avg_emotion_accuracy:.3f}\")\n",
        "print(f\"   Precision: {avg_emotion_precision:.3f}\")\n",
        "print(f\"   Recall: {avg_emotion_recall:.3f}\")\n",
        "print(f\"   F1-score: {avg_emotion_f1:.3f}\")\n",
        "\n",
        "print(\"\\n Trigger classification:\")\n",
        "print(f\"   Accuracy: {avg_trigger_accuracy:.3f}\")\n",
        "print(f\"   Precision: {avg_trigger_precision:.3f}\")\n",
        "print(f\"   Recall: {avg_trigger_recall:.3f}\")\n",
        "print(f\"   F1-score: {avg_trigger_f1:.3f}\")"
      ],
      "metadata": {
        "id": "8UNspSJxUgLJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Load and test trained model"
      ],
      "metadata": {
        "id": "goQBmT0AUx-x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "moe_loaded = MoEForEmotionAndTriggerClassification(gate_type = GATE_TYPE, expert_type = EXPERT_TYPE, num_experts = NUM_EXPERTS, k = TOP_K, num_classes = len(labels_to_ids))\n",
        "moe_loaded.load_state_dict(torch.load(f'trained_models/{DATASET}/moe_model_{GATE_TYPE}_gate_{NUM_EXPERTS}_{EXPERT_TYPE}_experts_{TOP_K}_active_{LEARNING_RATE}_lr_{NUM_EPOCHS}_epochs.pth', map_location=torch.device('cpu')))"
      ],
      "metadata": {
        "id": "Y8SZCnMSYfQU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d1b63cc4-a8c5-4f18-90d5-f174d69e007d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "<ipython-input-87-b016ff191b10>:2: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  moe_loaded.load_state_dict(torch.load(f'trained_models/moe_model_{GATE_TYPE}_gate_{NUM_EXPERTS}_{EXPERT_TYPE}_experts_{TOP_K}_active_{LEARNING_RATE}_lr_{NUM_EPOCHS}_epochs.pth', map_location=torch.device(\"cpu\")))\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {},
          "execution_count": 87
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "avg_emotion_accuracy, avg_emotion_precision, avg_emotion_recall, avg_emotion_f1, avg_trigger_accuracy, avg_trigger_precision, avg_trigger_recall, avg_trigger_f1 = get_metrics(moe_loaded, test_loader, 'cpu')\n",
        "\n",
        "# Output results\n",
        "print(\"Emotion classification:\")\n",
        "print(f\"   Accuracy: {avg_emotion_accuracy:.3f}\")\n",
        "print(f\"   Precision: {avg_emotion_precision:.3f}\")\n",
        "print(f\"   Recall: {avg_emotion_recall:.3f}\")\n",
        "print(f\"   F1-score: {avg_emotion_f1:.3f}\")\n",
        "\n",
        "print(\"\\n Trigger classification:\")\n",
        "print(f\"   Accuracy: {avg_trigger_accuracy:.3f}\")\n",
        "print(f\"   Precision: {avg_trigger_precision:.3f}\")\n",
        "print(f\"   Recall: {avg_trigger_recall:.3f}\")\n",
        "print(f\"   F1-score: {avg_trigger_f1:.3f}\")"
      ],
      "metadata": {
        "id": "lrsLrQjgUxeB"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}