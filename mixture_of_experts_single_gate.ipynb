{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"4YS-HHlMxdzr"},"outputs":[],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"hMHoUEE8xvX0"},"outputs":[],"source":["%cd drive/MyDrive/Colab \\Notebooks"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"M6Woq9BFW18k"},"outputs":[],"source":["import torch\n","import torch.nn as nn\n","from torch.utils.data import Dataset, DataLoader\n","from transformers import BertTokenizer, BertModel\n","import pandas as pd\n","import numpy as np"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4QJ_vvYdW2YT"},"outputs":[],"source":["from torch import cuda\n","device = 'cuda' if cuda.is_available() else 'cpu'\n","\n","print(device)"]},{"cell_type":"markdown","metadata":{"id":"DWH--rE_zW7H"},"source":["# Data preparation"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"L86nTmkQXOp-"},"outputs":[],"source":["DATASET = 'MaSaC'    # @param ['MELD', 'MaSaC']\n","MAX_LENGTH = 128    # @param [96, 128, 256] {type: 'raw'}\n","BATCH_SIZE = 32    # @param [8, 16, 32] {type: 'raw'}"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"wGHW3dIODZaA"},"outputs":[],"source":["if DATASET == 'MELD':\n","    DESIGNATED_MODEL = 'bert-base-cased'\n","elif DATASET == 'MaSaC':\n","    DESIGNATED_MODEL = 'bert-base-multilingual-cased'"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Jeo5-OB-0ui1"},"outputs":[],"source":["def get_data(dataset_name, stage):\n","    def to_float(x):\n","        try:\n","            return float(x)\n","        except ValueError:\n","            return 1\n","\n","    df = pd.read_json(f'data/EDiReF_{stage}_data/{dataset_name}_{stage}_efr.json')\n","    df[\"triggers\"] = df[\"triggers\"].apply(lambda lst: [np.nan if x is None else x for x in lst])\n","    df = df[df[\"triggers\"].apply(lambda lst: not any(pd.isna(x) for x in lst))]\n","    df[\"triggers\"] = df[\"triggers\"].apply(lambda lst: [to_float(x) for x in lst])\n","\n","    conversations = list(df['utterances'])\n","    emotions = list(df['emotions'])\n","    triggers = list(df['triggers'])\n","\n","    return conversations, emotions, triggers"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ydKZCcuX0ui1"},"outputs":[],"source":["train_conversations, train_emotions, train_triggers = get_data(DATASET, 'train')\n","val_conversations, val_emotions, val_triggers = get_data(DATASET, 'val')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"wN64aPnJ0ui1"},"outputs":[],"source":["conversations = train_conversations + val_conversations\n","emotions = train_emotions + val_emotions\n","triggers = train_triggers + val_triggers"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9ivzCvXX0ui1"},"outputs":[],"source":["flattened_emotions = [sent for conv in emotions for sent in conv]\n","unique_emotions = set(flattened_emotions)\n","\n","labels_to_ids = {k: v for v, k in enumerate(unique_emotions)}\n","ids_to_labels = {v: k for v, k in enumerate(unique_emotions)}\n","emotions = [[labels_to_ids[emotion] for emotion in conv] for conv in emotions]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"mi-y0zznb-hG"},"outputs":[],"source":["from sklearn.model_selection import train_test_split\n","\n","def train_val_test_split(X, y1, y2, val_size = 0.2, test_size = 0.2, random_state = None):\n","    X_train_val, X_test, y1_train_val, y1_test, y2_train_val, y2_test = train_test_split(\n","        X, y1, y2, test_size=test_size, random_state=random_state\n","    )\n","\n","    val_relative_size = val_size / (1 - test_size)\n","\n","    X_train, X_val, y1_train, y1_val, y2_train, y2_val = train_test_split(\n","        X_train_val, y1_train_val, y2_train_val, test_size=val_relative_size, random_state=random_state\n","    )\n","\n","    return (X_train, X_val, X_test, y1_train, y1_val, y1_test, y2_train, y2_val, y2_test)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"arnKxaiEeoLD"},"outputs":[],"source":["X_train, X_val, X_test, y1_train, y1_val, y1_test, y2_train, y2_val, y2_test = train_val_test_split(\n","    conversations, emotions, triggers, test_size=0.15, val_size=0.15, random_state=2024\n","    )"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"8Tru4JRaGT8X"},"outputs":[],"source":["tokenizer = BertTokenizer.from_pretrained(DESIGNATED_MODEL)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ng_4Q4k1XJvc"},"outputs":[],"source":["def tokenize_conversation(conversations, max_length = 128):\n","    input_ids = []\n","    attention_masks = []\n","\n","    for conversation in conversations:\n","        dialogue = \" [SEP] \".join(conversation)\n","        encoded = tokenizer(\n","            dialogue,\n","            truncation = True,\n","            padding = 'max_length',\n","            max_length = max_length,\n","            return_tensors = \"pt\"\n","        )\n","        input_ids.append(encoded[\"input_ids\"].squeeze(0))\n","        attention_masks.append(encoded[\"attention_mask\"].squeeze(0))\n","\n","    return input_ids, attention_masks"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"kUdbsdw_XMrD"},"outputs":[],"source":["def pad_labels(labels, max_length = 128):\n","    padded_labels = []\n","    for label_set in labels:\n","        label_tensor = torch.tensor(label_set, dtype = torch.float)\n","        # Pad with -1 to ignore padding tokens in the loss function\n","        padded_tensor = torch.cat(\n","            [label_tensor, torch.full((max_length - len(label_set),), -1)]\n","        )\n","        padded_labels.append(padded_tensor)\n","    return padded_labels"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"8pcFsfoZXNBD"},"outputs":[],"source":["class ConversationDataset(Dataset):\n","    def __init__(self, input_ids, attention_masks, emotion_labels, trigger_labels):\n","        self.input_ids = input_ids\n","        self.attention_masks = attention_masks\n","        self.emotion_labels = emotion_labels\n","        self.trigger_labels = trigger_labels\n","\n","    def __len__(self):\n","        return len(self.input_ids)\n","\n","    def __getitem__(self, idx):\n","        return {\n","            \"input_ids\": self.input_ids[idx],\n","            \"attention_mask\": self.attention_masks[idx],\n","            \"emotion_labels\": self.emotion_labels[idx],\n","            \"trigger_labels\": self.trigger_labels[idx],\n","        }"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2XCr0QOc0ui4"},"outputs":[],"source":["def create_dataloader(conversations, emotions, triggers, max_length = 128):\n","    input_ids, attention_masks = tokenize_conversation(conversations, max_length = max_length)\n","    emotion_labels = pad_labels(emotions, max_length = max_length)\n","    trigger_labels = pad_labels(triggers, max_length = max_length)\n","\n","    dataset = ConversationDataset(input_ids, attention_masks, emotion_labels, trigger_labels)\n","    loader = DataLoader(dataset, batch_size = BATCH_SIZE, shuffle = False)\n","\n","    return loader"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"8a3-AwoX0ui4"},"outputs":[],"source":["train_loader = create_dataloader(X_train, y1_train, y2_train, max_length = MAX_LENGTH)\n","val_loader = create_dataloader(X_val, y1_val, y2_val, max_length = MAX_LENGTH)\n","test_loader = create_dataloader(X_test, y1_test, y2_test, max_length = MAX_LENGTH)"]},{"cell_type":"markdown","metadata":{"id":"X5MO3g0VztW_"},"source":["# Model configuration"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"kueFTpHSXViM"},"outputs":[],"source":["GATE_TYPE = 'linear'  # @param ['linear', 'mlp']\n","EXPERT_TYPE = 'linear' # @param ['linear', 'mlp', 'rnn']\n","NUM_EXPERTS = 2 # @param {type: 'slider', min: 1, max: 8, step: 1}\n","TOP_K = 2 # @param {type: 'slider', min: 1, max: 8, step: 1}"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"12upnrxiXXbM"},"outputs":[],"source":["assert TOP_K <= NUM_EXPERTS, \"Select different values for TOP_K and NUM_EXPERTS!\""]},{"cell_type":"code","execution_count":null,"metadata":{"id":"umr0EC9_XuzD"},"outputs":[],"source":["class MoEForEmotionAndTriggerClassification(nn.Module):\n","    def __init__(self, num_experts, k, num_classes, gate_type = 'linear', expert_type = 'linear', model_name = 'bert-base-uncased'):\n","        super(MoEForEmotionAndTriggerClassification, self).__init__()\n","\n","        self.model = BertModel.from_pretrained(model_name)\n","        for param in self.model.parameters():\n","            param.requires_grad = True  # Set to True if you want to fine-tune model\n","        hidden_size = self.model.config.hidden_size\n","\n","        gate_setup = {\n","            'linear': nn.Linear(hidden_size, num_experts),\n","            'mlp': nn.Sequential(nn.Linear(hidden_size, 512), nn.ReLU(), nn.Linear(512, num_experts)),\n","        }\n","\n","        expert_setup = {\n","            'linear': nn.Linear(hidden_size, hidden_size),\n","            'mlp': nn.Sequential(nn.Linear(hidden_size, 512), nn.ReLU(), nn.Linear(512, hidden_size)),\n","            'rnn': nn.LSTM(hidden_size, hidden_size),\n","        }\n","\n","        self.gating_network = gate_setup[gate_type]\n","        self.experts = nn.ModuleList([expert_setup[expert_type] for _ in range(num_experts)])\n","\n","        self.emotion_classifier = nn.Linear(hidden_size, num_classes)\n","        self.trigger_classifier = nn.Linear(hidden_size, 1)\n","\n","        self.k = k\n","        self.dropout = nn.Dropout(p = 0.1)\n","\n","    def forward(self, input_ids, attention_mask):\n","        model_outputs = self.model(input_ids = input_ids, attention_mask = attention_mask)\n","        embeddings = model_outputs.last_hidden_state  # (batch_size, seq_len, hidden_size)\n","        pooled_embeddings = embeddings.mean(dim = 1)    # (batch_size, hidden_size)\n","        pooled_embeddings = self.dropout(pooled_embeddings)\n","\n","        # expert weights\n","        expert_weights = self.gating_network(pooled_embeddings) # (batch_size, num_experts)\n","        expert_weights = torch.softmax(expert_weights, dim = -1)\n","\n","        # aggregate expert outputs\n","        combined_output = self._compute_expert_output(embeddings, expert_weights)\n","        combined_output = self.dropout(combined_output)\n","\n","        emotion_logits = self.emotion_classifier(combined_output)   # (batch_size, seq_len, num_classes)\n","        trigger_logits = self.trigger_classifier(combined_output).squeeze(-1)   # (batch_size, seq_len)\n","\n","        return emotion_logits, trigger_logits\n","\n","    def _compute_expert_output(self, embeddings, expert_weights):\n","        combined_output = torch.zeros_like(embeddings)\n","\n","        # top-k experts only are activated\n","        topk_weights, topk_indices = torch.topk(expert_weights, self.k, dim = -1)\n","\n","        for i in range(self.k):\n","            expert_idx = topk_indices[:, i]\n","            weight = topk_weights[:, i].unsqueeze(-1).unsqueeze(-1)\n","\n","            expert_outputs = []\n","            for j in range(expert_idx.size(0)):\n","                expert = self.experts[expert_idx[j]]\n","\n","                if isinstance(expert, nn.LSTM):\n","                    embedding_input = embeddings[j].unsqueeze(0)\n","                    output, _ = expert(embedding_input)\n","                    expert_outputs.append(output.squeeze(0))\n","\n","                elif isinstance(expert, nn.Linear) or isinstance(expert, nn.Sequential):\n","                    output = expert(embeddings[j])\n","                    expert_outputs.append(output)\n","\n","            expert_outputs = torch.stack(expert_outputs)\n","            combined_output += weight * expert_outputs\n","\n","        return combined_output"]},{"cell_type":"markdown","metadata":{"id":"Fpyq3mUpz6D2"},"source":["# Training parameters"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"eO2uRZl2zxvO"},"outputs":[],"source":["LEARNING_RATE = 0.00002  # @param {type: 'slider', min: 1E-5, max: 5E-5, step: 1E-5}\n","NUM_EPOCHS = 5  # @param {type: 'slider', min: 3, max: 15, step: 1}\n","WEIGHT_TRIGGERS = True # @param {type: 'boolean'}"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"oJPBM0OaYWBf"},"outputs":[],"source":["from torch.optim import AdamW\n","from torch.nn import CrossEntropyLoss, BCEWithLogitsLoss\n","\n","moe = MoEForEmotionAndTriggerClassification(num_experts = NUM_EXPERTS, k = TOP_K, num_classes = len(labels_to_ids), gate_type = GATE_TYPE, expert_type = EXPERT_TYPE, model_name = DESIGNATED_MODEL)\n","optimizer = AdamW(moe.parameters(), lr = LEARNING_RATE)\n","\n","pos_weight = None\n","if WEIGHT_TRIGGERS and DATASET == 'MaSaC':\n","    flattened_triggers = [x for y in y2_train for x in y]\n","    num_negative_samples = len([x for x in flattened_triggers if x == 0])*0.8\n","    num_positive_samples = len(flattened_triggers) - num_negative_samples\n","    pos_weight_value = num_negative_samples / num_positive_samples\n","    pos_weight = torch.tensor([pos_weight_value], device = device)\n","\n","emotion_loss_fn = CrossEntropyLoss()\n","trigger_loss_fn = BCEWithLogitsLoss(pos_weight = pos_weight)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"EiLsS9Y9YWjk"},"outputs":[],"source":["moe.to(device)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"44x3zphNQWrq"},"outputs":[],"source":["def remove_padding(logits, labels, task):\n","    mask = labels != -1\n","\n","    logits_flat = logits.view(-1, logits.size(-1)) if task == 'emotion' else logits.view(-1)\n","    labels_flat = labels.view(-1)\n","\n","    logits = logits_flat[mask.view(-1)]\n","    labels = labels_flat[mask.view(-1)]\n","\n","    return logits, labels"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7QS5wNkiYZt7"},"outputs":[],"source":["def evaluate(model, val_loader):\n","    model.eval()\n","    val_loss, nb_steps = 0.0, 0\n","    total_emotion_preds, correct_emotion_preds = 0, 0\n","    total_trigger_preds, correct_trigger_preds = 0, 0\n","    val_logs = []\n","\n","    with torch.no_grad():\n","        for idx, batch in enumerate(val_loader):\n","            input_ids = batch['input_ids'].to(device)\n","            attention_mask = batch['attention_mask'].to(device)\n","            emotion_labels = batch['emotion_labels'].to(device)\n","            trigger_labels = batch['trigger_labels'].to(device)\n","\n","            emotion_logits, trigger_logits = model(input_ids, attention_mask)\n","\n","            # removing padding\n","            emotion_logits, emotion_labels = remove_padding(emotion_logits, emotion_labels, 'emotion')\n","            trigger_logits, trigger_labels = remove_padding(trigger_logits, trigger_labels, 'trigger')\n","\n","            # calculating loss\n","            emotion_loss = emotion_loss_fn(emotion_logits, emotion_labels.long())\n","            trigger_loss = trigger_loss_fn(trigger_logits, trigger_labels)\n","\n","            loss = emotion_loss + trigger_loss\n","            val_loss += loss.item()\n","\n","            # calculating accuracy\n","            emotion_preds = torch.argmax(emotion_logits, dim=-1)\n","            trigger_preds = (torch.sigmoid(trigger_logits).squeeze(-1) > 0.5).long()\n","\n","            correct_emotion_preds += torch.sum(emotion_preds == emotion_labels).item()\n","            correct_trigger_preds += torch.sum(trigger_preds == trigger_labels).item()\n","\n","            total_emotion_preds += emotion_labels.numel()\n","            total_trigger_preds += trigger_labels.numel()\n","\n","            nb_steps += 1\n","\n","            if idx % 100 == 0:\n","                loss_step = val_loss / nb_steps\n","                print(f'      Validation loss per 100 training steps: {loss_step}')\n","                val_logs.append(f'      Validation loss per 100 training steps: {loss_step}\\n')\n","\n","        avg_val_loss = val_loss / len(val_loader)\n","        emotion_accuracy = correct_emotion_preds / total_emotion_preds\n","        trigger_accuracy = correct_trigger_preds / total_trigger_preds\n","        avg_val_accuracy = (emotion_accuracy + trigger_accuracy)/2\n","\n","    return avg_val_loss, avg_val_accuracy, val_logs"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6Cq8fwysYaY_"},"outputs":[],"source":["def train_and_validate(model, train_loader, val_loader, num_epochs = 3):\n","    train_logs = []\n","\n","    for epoch in range(num_epochs):\n","        print(f\"Epoch [{epoch + 1}/{num_epochs}]\")\n","        train_logs.append(f\"Epoch [{epoch + 1}/{num_epochs}]\\n\")\n","        model.train()\n","        train_loss, nb_steps = 0.0, 0\n","        total_emotion_preds, correct_emotion_preds = 0, 0\n","        total_trigger_preds, correct_trigger_preds = 0, 0\n","\n","        for idx, batch in enumerate(train_loader):\n","            optimizer.zero_grad()\n","\n","            input_ids = batch['input_ids'].to(device)\n","            attention_mask = batch['attention_mask'].to(device)\n","            emotion_labels = batch['emotion_labels'].to(device)\n","            trigger_labels = batch['trigger_labels'].to(device)\n","\n","            emotion_logits, trigger_logits = model(input_ids, attention_mask)\n","\n","            # removing padding\n","            emotion_logits, emotion_labels = remove_padding(emotion_logits, emotion_labels, 'emotion')\n","            trigger_logits, trigger_labels = remove_padding(trigger_logits, trigger_labels, 'trigger')\n","\n","            # calculating loss\n","            emotion_loss = emotion_loss_fn(emotion_logits, emotion_labels.long())\n","            trigger_loss = trigger_loss_fn(trigger_logits, trigger_labels)\n","\n","            loss = emotion_loss + trigger_loss\n","            train_loss += loss.item()\n","\n","            loss.backward()\n","            optimizer.step()\n","\n","            # calculating accuracy\n","            emotion_preds = torch.argmax(emotion_logits, dim=-1)\n","            trigger_preds = (torch.sigmoid(trigger_logits).squeeze(-1) > 0.5).long()\n","\n","            correct_emotion_preds += torch.sum(emotion_preds == emotion_labels).item()\n","            correct_trigger_preds += torch.sum(trigger_preds == trigger_labels).item()\n","\n","            total_emotion_preds += emotion_labels.numel()\n","            total_trigger_preds += trigger_labels.numel()\n","            nb_steps += 1\n","\n","            if idx % 100 == 0:\n","                loss_step = train_loss / nb_steps\n","                print(f'      Training loss per 100 training steps: {loss_step}')\n","                train_logs.append(f'      Training loss per 100 training steps: {loss_step}\\n')\n","\n","        avg_train_loss = train_loss / len(train_loader)\n","        emotion_accuracy = correct_emotion_preds / total_emotion_preds\n","        trigger_accuracy = correct_trigger_preds / total_trigger_preds\n","        avg_train_accuracy = (emotion_accuracy + trigger_accuracy)/2\n","\n","        val_loss, val_accuracy, val_logs = evaluate(model, val_loader)\n","        train_logs.extend(val_logs)\n","\n","        print(f\"   Training Loss: {avg_train_loss:.3f}, Training Accuracy: {avg_train_accuracy:.3f}\")\n","        train_logs.append(f\"   Training Loss: {avg_train_loss:.3f}, Training Accuracy: {avg_train_accuracy:.3f}\\n\")\n","\n","        print(f\"   Validation Loss: {val_loss:.3f}, Validation Accuracy: {val_accuracy:.3f}\\n\")\n","        train_logs.append(f\"   Validation Loss: {val_loss:.3f}, Validation Accuracy: {val_accuracy:.3f}\\n\\n\")\n","\n","    return train_logs"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"N_U28Fs-YcFL"},"outputs":[],"source":["logs = train_and_validate(moe, train_loader, val_loader, num_epochs = NUM_EPOCHS)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2To3TjUdUEsR"},"outputs":[],"source":["from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score, confusion_matrix\n","from collections import defaultdict\n","\n","def get_metrics(model, data_loader, dev):\n","    model.eval()\n","\n","    emotion_accuracy = 0.0\n","    emotion_precision = 0.0\n","    emotion_recall = 0.0\n","    emotion_f1 = 0.0\n","    emotion_cm = None\n","\n","    unique_emotions_f1 = dict.fromkeys(labels_to_ids.keys(), 0.0)\n","\n","    trigger_accuracy = 0.0\n","    trigger_precision = 0.0\n","    trigger_recall = 0.0\n","    trigger_f1 = 0.0\n","    trigger_cm = None\n","\n","    nb_steps = 0\n","\n","    for batch in data_loader:\n","        input_ids = batch['input_ids'].to(dev)\n","        attention_mask = batch['attention_mask'].to(dev)\n","        emotion_labels = batch['emotion_labels'].to(dev)\n","        trigger_labels = batch['trigger_labels'].to(dev)\n","\n","        with torch.no_grad():\n","            # Forward pass\n","            emotion_logits, trigger_logits = model(input_ids, attention_mask)\n","\n","            # Compute predictions for emotions\n","            emotion_logits, emotion_labels = remove_padding(emotion_logits, emotion_labels, 'emotion')\n","\n","            emotion_preds = torch.argmax(emotion_logits, dim = -1)\n","\n","            emotion_preds_flat = emotion_preds.cpu().numpy()\n","            emotion_labels_flat = emotion_labels.cpu().numpy()\n","\n","            # Compute predictions for triggers\n","            trigger_logits, trigger_labels = remove_padding(trigger_logits, trigger_labels, 'trigger')\n","\n","            trigger_preds = (torch.sigmoid(trigger_logits).squeeze(-1) > 0.5).long()\n","\n","            trigger_preds_flat = trigger_preds.cpu().numpy()\n","            trigger_labels_flat = trigger_labels.cpu().numpy()\n","\n","            # Calculate metrics for emotion classification\n","            emotion_accuracy += accuracy_score(emotion_labels_flat, emotion_preds_flat)\n","            emotion_precision += precision_score(emotion_labels_flat, emotion_preds_flat, average='weighted', zero_division = 0)\n","            emotion_recall += recall_score(emotion_labels_flat, emotion_preds_flat, average='weighted', zero_division = 0)\n","            emotion_f1 += f1_score(emotion_labels_flat, emotion_preds_flat, average='weighted', zero_division = 0)\n","\n","            for idx, score in enumerate(f1_score(emotion_labels_flat, emotion_preds_flat, average = None, zero_division = 0)):\n","                unique_emotions_f1[ids_to_labels[idx]] += score\n","\n","            if emotion_cm is None:\n","                emotion_cm = confusion_matrix(emotion_labels_flat, emotion_preds_flat, labels = range(len(labels_to_ids)))\n","            else:\n","                emotion_cm += confusion_matrix(emotion_labels_flat, emotion_preds_flat, labels = range(len(labels_to_ids)))\n","\n","            # Calculate metrics for trigger classification\n","            trigger_accuracy += accuracy_score(trigger_labels_flat, trigger_preds_flat)\n","            trigger_precision += precision_score(trigger_labels_flat, trigger_preds_flat, average='weighted', zero_division = 0)\n","            trigger_recall += recall_score(trigger_labels_flat, trigger_preds_flat, average='weighted', zero_division = 0)\n","            trigger_f1 += f1_score(trigger_labels_flat, trigger_preds_flat, average='weighted', zero_division = 0)\n","\n","            if trigger_cm is None:\n","                trigger_cm = confusion_matrix(trigger_labels_flat, trigger_preds_flat, labels = [0, 1])\n","            else:\n","                trigger_cm += confusion_matrix(trigger_labels_flat, trigger_preds_flat, labels = [0, 1])\n","\n","            nb_steps += 1\n","\n","    metrics = defaultdict(lambda: {})\n","\n","    # Calculate average metrics\n","    avg_emotion_accuracy = emotion_accuracy / nb_steps\n","    avg_emotion_precision = emotion_precision / nb_steps\n","    avg_emotion_recall = emotion_recall / nb_steps\n","    avg_emotion_f1 = emotion_f1 / nb_steps\n","\n","    for key, value in unique_emotions_f1.items():\n","        unique_emotions_f1[key] = value / nb_steps\n","\n","    metrics['emotion_classification'] = {'accuracy': avg_emotion_accuracy,\n","                                         'precision': avg_emotion_precision,\n","                                         'recall': avg_emotion_recall,\n","                                         'f1': {'avg': avg_emotion_f1}}\n","    metrics['emotion_classification']['f1'].update(unique_emotions_f1)\n","\n","    avg_trigger_accuracy = trigger_accuracy / nb_steps\n","    avg_trigger_precision = trigger_precision / nb_steps\n","    avg_trigger_recall = trigger_recall / nb_steps\n","    avg_trigger_f1 = trigger_f1 / nb_steps\n","\n","    metrics['trigger_classification'] = {'accuracy': avg_trigger_accuracy,\n","                                         'precision': avg_trigger_precision,\n","                                         'recall': avg_trigger_recall,\n","                                         'f1': avg_trigger_f1}\n","\n","    return metrics, emotion_cm, trigger_cm"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"m2819G500ui7"},"outputs":[],"source":["import seaborn as sns\n","import matplotlib.pyplot as plt\n","\n","def plot_confusion_matrix(cm, labels, title = \"Confusion Matrix\"):\n","    plt.figure(figsize = (10, 8))\n","    sns.heatmap(cm, annot = True, fmt = 'd', cmap = \"Blues\", xticklabels = labels, yticklabels = labels)\n","    plt.xlabel('Predicted Labels', fontsize = 14)\n","    plt.ylabel('True Labels', fontsize = 14)\n","    plt.title(title, fontsize = 16)\n","    plt.xticks(rotation = 90)\n","    plt.yticks(rotation = 0)\n","    plt.tight_layout()\n","    plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"8UNspSJxUgLJ"},"outputs":[],"source":["metrics, emotion_cm, trigger_cm = get_metrics(moe, test_loader, device)\n","\n","# Output results\n","for task, results in metrics.items():\n","    print(f'Task: {task}')\n","    for metric, score in results.items():\n","        if metric != 'f1' or (metric == 'f1' and isinstance(score, np.float64)):\n","            print(f'      {metric}: {score:.3f}')\n","        else:\n","            print(f'      f1: ')\n","            for x, y in score.items():\n","                print(f'          {x}: {y:.3f}')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"vu-QJEmJ0ui7"},"outputs":[],"source":["plot_confusion_matrix(emotion_cm, [i for i in labels_to_ids.keys()], \"Emotion Classification\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"85ShzciV0ui7"},"outputs":[],"source":["plot_confusion_matrix(trigger_cm, ['No trigger', 'Trigger'], \"Trigger Classification\")"]},{"cell_type":"markdown","metadata":{"id":"qiwgK0H60ui8"},"source":["# Save experiment"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"s7hH0xNVYdqL"},"outputs":[],"source":["# torch.save(moe.state_dict(), f'trained_models/{DATASET}/moe_model_{GATE_TYPE}_single_gate_{NUM_EXPERTS}_{EXPERT_TYPE}_experts_{TOP_K}_active_{LEARNING_RATE}_lr_{NUM_EPOCHS}_epochs.pth')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"wwX2HcS0vQQr"},"outputs":[],"source":["def write_confusion_matrix(title, cm, labels):\n","    cm2txt = []\n","    col_width = max(len(max(labels, key=len)), 10)\n","    cm2txt.append('\\n' + title + '\\n')\n","\n","    header = f\"{'':<{col_width}}\"\n","    header += \"\".join([f\"{label:<{col_width}}\" for label in labels])\n","\n","    cm2txt.append(header + '\\n')\n","\n","    rows = []\n","    for i, label in enumerate(labels):\n","        row = f\"{label:<{col_width}}\"\n","        row += \"\".join([f\"{value:<{col_width}}\" for value in cm[i]])\n","        rows.append(row)\n","\n","    cm2txt.append(\"\\n\".join(rows))\n","\n","    return cm2txt"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"cR5h2JI40ui8"},"outputs":[],"source":["if WEIGHT_TRIGGERS and DATASET == 'MaSaC':\n","    file_path = f'results/{DATASET}/weighted_moe_model_{GATE_TYPE}_single_gate_{NUM_EXPERTS}_{EXPERT_TYPE}_experts_{TOP_K}_active_{LEARNING_RATE}_lr_{NUM_EPOCHS}_epochs.txt'\n","else:\n","    file_path = f'results/{DATASET}/moe_model_{GATE_TYPE}_single_gate_{NUM_EXPERTS}_{EXPERT_TYPE}_experts_{TOP_K}_active_{LEARNING_RATE}_lr_{NUM_EPOCHS}_epochs.txt'\n","\n","with open(file_path, 'w') as f:\n","    experiment_setup = [f'MAX_LENGTH = {MAX_LENGTH}\\n',\n","                        f'BATCH_SIZE = {BATCH_SIZE}\\n', '\\n',\n","                        f'GATE_TYPE = {GATE_TYPE}\\n',\n","                        f'EXPERT_TYPE = {EXPERT_TYPE}\\n',\n","                        f'NUM_EXPERTS = {NUM_EXPERTS}\\n',\n","                        f'TOP_K = {TOP_K}\\n',\n","                        f'LEARNING_RATE = {LEARNING_RATE}\\n',\n","                        f'NUM_EPOCHS = {NUM_EPOCHS}\\n', '\\n', '\\n']\n","\n","    experiment_results = []\n","    for task, results in metrics.items():\n","        experiment_results.append(f'\\nTask: {task}\\n')\n","        for metric, score in results.items():\n","            if metric != 'f1' or (metric == 'f1' and isinstance(score, np.float64)):\n","                experiment_results.append(f'      {metric}: {score:.3f}\\n')\n","            else:\n","                experiment_results.append(f'      f1:\\n')\n","                for x, y in score.items():\n","                    experiment_results.append(f'          {x}: {y:.3f}\\n')\n","\n","    f.writelines(experiment_setup)\n","    f.writelines(logs)\n","    f.writelines(experiment_results)\n","    f.write(\"\\n\")\n","    writeable_emotion_cm = write_confusion_matrix('Emotion confusion matrix', emotion_cm, [i for i in labels_to_ids.keys()])\n","    f.writelines(writeable_emotion_cm)\n","    f.write(\"\\n\")\n","    writeable_trigger_cm = write_confusion_matrix('Trigger confusion matrix', trigger_cm, ['No trigger', 'Trigger'])\n","    f.writelines(writeable_trigger_cm)"]},{"cell_type":"markdown","metadata":{"id":"goQBmT0AUx-x"},"source":["# Load and test trained model"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Y8SZCnMSYfQU"},"outputs":[],"source":["# moe_loaded = MoEForEmotionAndTriggerClassification(num_experts = NUM_EXPERTS, k = TOP_K, num_classes = len(labels_to_ids), gate_type = GATE_TYPE, expert_type = EXPERT_TYPE)\n","# moe_loaded.load_state_dict(torch.load(f'trained_models/{DATASET}/moe_model_{GATE_TYPE}_single_gate_{NUM_EXPERTS}_{EXPERT_TYPE}_experts_{TOP_K}_active_{LEARNING_RATE}_lr_{NUM_EPOCHS}_epochs.pth', map_location=torch.device('cpu')))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"lrsLrQjgUxeB"},"outputs":[],"source":["# metrics, emotion_cm, trigger_cm = get_metrics(moe_loaded, test_loader, 'cpu')\n","\n","# # Output results\n","# for task, results in metrics.items():\n","#     print(f'Task: {task}')\n","#     for metric, score in results.items():\n","#         if metric != 'f1' or (metric == 'f1' and isinstance(score, np.float64)):\n","#             print(f'      {metric}: {score:.3f}')\n","#         else:\n","#             print(f'      f1: ')\n","#             for x, y in score.items():\n","#                 print(f'          {x}: {y:.3f}')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"O3_2OzBi0ui9"},"outputs":[],"source":["# plot_confusion_matrix(emotion_cm, [i for i in labels_to_ids.keys()], \"Emotion Classification\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"mQU_IDQ90ui9"},"outputs":[],"source":["# plot_confusion_matrix(trigger_cm, ['No trigger', 'Trigger'], \"Trigger Classification\")"]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[]},"kernelspec":{"display_name":"base","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.5"}},"nbformat":4,"nbformat_minor":0}