{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Mixture of Experts Model with Single Gate for Emotion & Trigger Classification**\n",
    "\n",
    "This notebook developed as part of Master's thesis runs training and evaluation of a single-gated Mixture of Experts model for joint emotion and trigger classification using BERT-based features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Imports & Setup**\n",
    "\n",
    "This section loads all necessary libraries. The code is designed to run on Google Colab with GPU support."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "M6Woq9BFW18k"
   },
   "outputs": [],
   "source": [
    "# Standard libraries\n",
    "from collections import defaultdict\n",
    "from collections.abc import Callable\n",
    "import os\n",
    "import random\n",
    "from typing import Any, Dict, List, Tuple, Union\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Data processing\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# PyTorch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import cuda\n",
    "from torch.nn import CrossEntropyLoss, BCEWithLogitsLoss\n",
    "from torch.optim import AdamW\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# Transformers\n",
    "from transformers import BertTokenizer, BertModel\n",
    "\n",
    "# Scikit-learn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import (\n",
    "    precision_score, \n",
    "    recall_score, \n",
    "    f1_score, \n",
    "    accuracy_score, \n",
    "    confusion_matrix,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4YS-HHlMxdzr"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount(\"/content/drive\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hMHoUEE8xvX0"
   },
   "outputs": [],
   "source": [
    "%cd \"drive/MyDrive/Colab Notebooks/EDiReF\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed(seed: int = 2024) -> None:\n",
    "    \"\"\"\n",
    "    Set random seeds for reproducibility across NumPy, PyTorch, and CUDA.\n",
    "\n",
    "    Args:\n",
    "        seed (int): A seed value to ensure deterministic behaviour.\n",
    "    \"\"\"\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_seed()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4QJ_vvYdW2YT"
   },
   "outputs": [],
   "source": [
    "device = \"cuda\" if cuda.is_available() else \"cpu\"\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DWH--rE_zW7H"
   },
   "source": [
    "### **Data Loading & Preprocessing**\n",
    "\n",
    "This section loads and preprocesses the conversation, emotion, and trigger label data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Input Format\n",
    "\n",
    "Each dataset (e.g. MELD, MaSaC) should be stored as a `.json` file in the `data/` directory.\n",
    "\n",
    "Each file must contain:\n",
    "- `\"utterances\"`: list of utterances per conversation (List[List[str]])\n",
    "- `\"emotions\"`: corresponding emotion labels (List[List[str]])\n",
    "- `\"triggers\"`: corresponding trigger indicators, binary or float (List[List[Union[int, float]]])\n",
    "\n",
    "Files should be named like:\n",
    "`data/EDiReF_train_data/MaSaC_train_efr.json`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "L86nTmkQXOp-"
   },
   "outputs": [],
   "source": [
    "dataset = \"MELD\"    # @param [\"MELD\", \"MaSaC\"]\n",
    "max_length = 96    # @param [96, 128, 256] {type: \"raw\"}\n",
    "batch_size = 8    # @param [8, 16, 32] {type: \"raw\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wGHW3dIODZaA"
   },
   "outputs": [],
   "source": [
    "if dataset == \"MELD\":\n",
    "    designated_model = \"bert-base-cased\"\n",
    "elif dataset == \"MaSaC\":\n",
    "    designated_model = \"bert-base-multilingual-cased\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Jeo5-OB-0ui1"
   },
   "outputs": [],
   "source": [
    "def get_data(\n",
    "    dataset_name: str, \n",
    "    stage: str\n",
    ") -> Tuple[List[List[str]], List[List[str]], List[List[float]]]:\n",
    "    \"\"\"\n",
    "    Load and clean EDiReF JSON data for a given dataset and stage.\n",
    "\n",
    "    The function loads a JSON file, fills missing values in the \"triggers\" column, \n",
    "    filters out invalid rows, and extracts \"utterances\", \"emotions\", and \"triggers\".\n",
    "\n",
    "    Args:\n",
    "        dataset_name (str): Name of the dataset (\"MELD\", or \"MaSaC\").\n",
    "        stage (str): Subset name corresponding to the file suffix (\"train\" or \"val\").\n",
    "\n",
    "    Returns:\n",
    "        Tuple[List[List[str]], List[List[str]], List[List[float]]]: A tuple containing:\n",
    "            - conversations: List of conversations (utterances).\n",
    "            - emotions: List of corresponding emotion labels.\n",
    "            - triggers: List of corresponding trigger values.\n",
    "    \"\"\"\n",
    "    def to_float(x):\n",
    "        try:\n",
    "            return float(x)\n",
    "        except ValueError:\n",
    "            return 1.0\n",
    "\n",
    "    df = pd.read_json(f\"data/EDiReF_{stage}_data/{dataset_name}_{stage}_efr.json\")\n",
    "    df[\"triggers\"] = df[\"triggers\"].apply(\n",
    "        lambda lst: [np.nan if x is None else x for x in lst]\n",
    "    )\n",
    "    df = df[df[\"triggers\"].apply(lambda lst: not any(pd.isna(x) for x in lst))]\n",
    "    df[\"triggers\"] = df[\"triggers\"].apply(\n",
    "        lambda lst: [to_float(x) for x in lst]\n",
    "    )\n",
    "\n",
    "    conversations = list(df[\"utterances\"])\n",
    "    emotions = list(df[\"emotions\"])\n",
    "    triggers = list(df[\"triggers\"])\n",
    "\n",
    "    return conversations, emotions, triggers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ydKZCcuX0ui1"
   },
   "outputs": [],
   "source": [
    "train_conversations, train_emotions, train_triggers = get_data(dataset, \"train\")\n",
    "val_conversations, val_emotions, val_triggers = get_data(dataset, \"val\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wN64aPnJ0ui1"
   },
   "outputs": [],
   "source": [
    "conversations = train_conversations + val_conversations\n",
    "emotions = train_emotions + val_emotions\n",
    "triggers = train_triggers + val_triggers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9ivzCvXX0ui1"
   },
   "outputs": [],
   "source": [
    "flattened_emotions = [sent for conv in emotions for sent in conv]\n",
    "unique_emotions = set(flattened_emotions)\n",
    "\n",
    "labels_to_ids = {k: v for v, k in enumerate(unique_emotions)}\n",
    "ids_to_labels = {v: k for v, k in enumerate(unique_emotions)}\n",
    "emotions = [[labels_to_ids[emotion] for emotion in conv] for conv in emotions]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mi-y0zznb-hG"
   },
   "outputs": [],
   "source": [
    "def train_val_test_split(\n",
    "    X: List[List[str]], \n",
    "    y1: List[List[int]], \n",
    "    y2: List[List[float]], \n",
    "    val_size: float = 0.2, \n",
    "    test_size: float = 0.2, \n",
    "    random_state: int = None\n",
    ") -> Tuple[\n",
    "    List[List[str]], List[List[str]], List[List[str]],\n",
    "    List[List[int]], List[List[int]], List[List[int]],\n",
    "    List[List[float]], List[List[float]], List[List[float]]\n",
    "]:\n",
    "    \"\"\"\n",
    "    Split data into train, validation, and test sets with consistent label alignment.\n",
    "\n",
    "    This function performs a two-step split:\n",
    "    - First, it splits the dataset into training+validation and test sets.\n",
    "    - Then it splits the training+validation set again to separate out a validation set.\n",
    "    This ensures y1 and y2 (labels) stay aligned with X throughout the process.\n",
    "\n",
    "    Args:\n",
    "        X (List[List[str]]): The main input data (e.g., tokenized conversations).\n",
    "        y1 (List[List[int]]): The first set of target labels (e.g., emotions).\n",
    "        y2 (List[List[float]]): The second set of target labels (e.g., triggers).\n",
    "        val_size (float, optional): Proportion of data to use for validation. \n",
    "            Defaults to 0.2.\n",
    "        test_size (float, optional): Proportion of data to use for test set. \n",
    "            Defaults to 0.2.\n",
    "        random_state (int, optional): Random seed for reproducibility. \n",
    "            Defaults to None.\n",
    "\n",
    "    Returns:\n",
    "        Tuple[\n",
    "            List[List[str]], List[List[str]], List[List[str]],\n",
    "            List[List[int]], List[List[int]], List[List[int]],\n",
    "            List[List[float]], List[List[float]], List[List[float]]\n",
    "        ]: A tuple containing:\n",
    "            - X_train, X_val, X_test\n",
    "            - y1_train, y1_val, y1_test\n",
    "            - y2_train, y2_val, y2_test\n",
    "    \"\"\"\n",
    "    X_train_val, X_test, y1_train_val, y1_test, y2_train_val, y2_test = train_test_split(\n",
    "        X, y1, y2, test_size=test_size, random_state=random_state\n",
    "    )\n",
    "\n",
    "    val_relative_size = val_size / (1 - test_size)\n",
    "\n",
    "    X_train, X_val, y1_train, y1_val, y2_train, y2_val = train_test_split(\n",
    "        X_train_val, y1_train_val, y2_train_val, \n",
    "        test_size=val_relative_size, \n",
    "        random_state=random_state\n",
    "    )\n",
    "\n",
    "    return (\n",
    "        X_train, X_val, X_test, \n",
    "        y1_train, y1_val, y1_test, \n",
    "        y2_train, y2_val, y2_test\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "arnKxaiEeoLD"
   },
   "outputs": [],
   "source": [
    "X_train, X_val, X_test, y1_train, y1_val, y1_test, y2_train, y2_val, y2_test = train_val_test_split(\n",
    "    conversations, emotions, triggers, test_size=0.15, val_size=0.15, random_state=2024\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Tokenization & Padding**\n",
    "\n",
    "Utterances are tokenized using the HuggingFace `BertTokenizer`. Labels are padded to match max sequence length with `-1`, which is ignored during loss calculation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8Tru4JRaGT8X"
   },
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained(designated_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ng_4Q4k1XJvc"
   },
   "outputs": [],
   "source": [
    "def tokenize_conversation(\n",
    "    conversations: List[List[str]], \n",
    "    tokenizer: BertTokenizer, \n",
    "    max_length: int = 128\n",
    ") -> Tuple[List[torch.Tensor], List[torch.Tensor]]:\n",
    "    \"\"\"\n",
    "    Tokenize and pad a list of conversations using pretrained BertTokenizer.\n",
    "\n",
    "    Each conversation is flattened into a single string using [SEP] as a delimiter.\n",
    "    Tokenization is performed with padding and truncation to a specified max length.\n",
    "\n",
    "    Args:\n",
    "        conversations (List[List[str]]): A list of conversations, where each\n",
    "            conversation is a list of utterances (strings).\n",
    "        tokenizer (BertTokenizer): A HuggingFace tokenizer used to tokenize\n",
    "            the input conversations.\n",
    "        max_length (int, optional): Maximum length (in tokens) for padding/truncation.\n",
    "            Defaults to 128.\n",
    "\n",
    "    Returns:\n",
    "        Tuple[List[torch.Tensor], List[torch.Tensor]]: A tuple containing:\n",
    "            - input_ids: Token ID tensors for each conversation.\n",
    "            - attention_masks: Attention mask tensors for each conversation.\n",
    "    \"\"\"\n",
    "    input_ids = []\n",
    "    attention_masks = []\n",
    "\n",
    "    for conversation in conversations:\n",
    "        dialogue = f\" {tokenizer.sep_token} \".join(conversation)\n",
    "        encoded = tokenizer(\n",
    "            dialogue,\n",
    "            truncation=True,\n",
    "            padding=\"max_length\",\n",
    "            max_length=max_length,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        input_ids.append(encoded[\"input_ids\"].squeeze(0))\n",
    "        attention_masks.append(encoded[\"attention_mask\"].squeeze(0))\n",
    "\n",
    "    return input_ids, attention_masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kUdbsdw_XMrD"
   },
   "outputs": [],
   "source": [
    "def pad_labels(\n",
    "    labels: List[List[Union[int, float]]], \n",
    "    max_length: int = 128\n",
    ") -> List[torch.Tensor]:\n",
    "    \"\"\"\n",
    "    Pads each list of labels to a specified max length.\n",
    "\n",
    "    Each list of labels is converted to a float tensor and padded with -1.0\n",
    "    to a specified max length. Useful for masking loss during training on\n",
    "    token-level tasks.\n",
    "\n",
    "    Args:\n",
    "        labels (List[List[Union[int, float]]]): A list of label sequences\n",
    "            (e.g., emotions or triggers).\n",
    "        max_length (int, optional): Maximum length to pad/truncate each\n",
    "            sequence to. Defaults to 128.\n",
    "\n",
    "    Returns:\n",
    "        List[torch.Tensor]: A list of padded 1D tensors, one per input sequence.\n",
    "    \"\"\"\n",
    "    padded_labels = []\n",
    "    \n",
    "    for label_set in labels:\n",
    "        label_tensor = torch.tensor(label_set, dtype=torch.float)\n",
    "        padding_tensor = torch.full((max_length - len(label_set),), -1.0)\n",
    "        padded_tensor = torch.cat([label_tensor, padding_tensor])\n",
    "        padded_labels.append(padded_tensor)\n",
    "\n",
    "    return padded_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8pcFsfoZXNBD"
   },
   "outputs": [],
   "source": [
    "class ConversationDataset(Dataset):\n",
    "    \"\"\"\n",
    "    A PyTorch-compatible dataset for emotion and trigger classification tasks.\n",
    "\n",
    "    Stores tokenized conversations along with attention masks, and emotion and \n",
    "    trigger labels for each utterance in the input.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self, \n",
    "        input_ids: List[torch.Tensor], \n",
    "        attention_masks: List[torch.Tensor], \n",
    "        emotion_labels: List[torch.Tensor], \n",
    "        trigger_labels: List[torch.Tensor]\n",
    "    ) -> None:\n",
    "        \"\"\"\n",
    "        Initialize the dataset with tokenized inputs and their corresponding labels.\n",
    "\n",
    "        Args:\n",
    "            input_ids (List[torch.Tensor]): Token IDs for each conversation.\n",
    "            attention_masks (List[torch.Tensor]): Attention masks for each conversation.\n",
    "            emotion_labels (List[torch.Tensor]): Label tensors for emotion classification.\n",
    "            trigger_labels (List[torch.Tensor]): Label tensors for trigger classification.\n",
    "        \"\"\"\n",
    "        self.input_ids = input_ids\n",
    "        self.attention_masks = attention_masks\n",
    "        self.emotion_labels = emotion_labels\n",
    "        self.trigger_labels = trigger_labels\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        \"\"\"\n",
    "        Return the number of samples in the dataset.\n",
    "\n",
    "        Returns:\n",
    "            int: The total number of data points.\n",
    "        \"\"\"\n",
    "        return len(self.input_ids)\n",
    "\n",
    "    def __getitem__(self, idx: int) -> Dict[str, torch.Tensor]:\n",
    "        \"\"\"\n",
    "        Retrieve a single item from the dataset.\n",
    "\n",
    "        Args:\n",
    "            idx (int): Index of the item to retrieve.\n",
    "\n",
    "        Returns:\n",
    "            Dict[str, torch.Tensor]: A dictionary containing:\n",
    "                - \"input_ids\"\n",
    "                - \"attention_mask\"\n",
    "                - \"emotion_labels\"\n",
    "                - \"trigger_labels\"\n",
    "        \"\"\"\n",
    "        return {\n",
    "            \"input_ids\": self.input_ids[idx],\n",
    "            \"attention_mask\": self.attention_masks[idx],\n",
    "            \"emotion_labels\": self.emotion_labels[idx],\n",
    "            \"trigger_labels\": self.trigger_labels[idx],\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2XCr0QOc0ui4"
   },
   "outputs": [],
   "source": [
    "def create_dataloader(\n",
    "    conversations: List[List[str]], \n",
    "    emotions: List[List[int]], \n",
    "    triggers: List[List[float]],\n",
    "    tokenizer: BertTokenizer,  \n",
    "    batch_size: int,\n",
    "    max_length: int = 128, \n",
    "    shuffle: bool = False\n",
    ") -> DataLoader:\n",
    "    \"\"\"\n",
    "    Create a DataLoader from conversations and corresponding labels.\n",
    "\n",
    "    Conversations are first tokenized using the `tokenize_conversation()` function,\n",
    "    and the emotion and trigger labels are padded using `pad_labels()`. These are then\n",
    "    wrapped in a `ConversationDataset` and returned as a PyTorch DataLoader.\n",
    "\n",
    "    Args:\n",
    "        conversations (List[List[str]]): A list of conversations, where each conversation\n",
    "            is a list of utterances (strings).\n",
    "        emotions (List[List[int]]): A list of emotion labels.\n",
    "        triggers (List[List[float]]): A list of trigger labels.\n",
    "        tokenizer (BertTokenizer): A HuggingFace tokenizer used to tokenize\n",
    "            the input conversations.\n",
    "        batch_size (int): Number of samples per batch.\n",
    "        max_length (int, optional): Maximum length to pad/truncate each sequence to.\n",
    "            Defaults to 128.\n",
    "        shuffle (bool, optional): Whether to shuffle the data at every epoch.\n",
    "            Defaults to False.\n",
    "\n",
    "    Returns:\n",
    "        DataLoader: An iterable over the constructed ConversationDataset.\n",
    "    \"\"\"\n",
    "    input_ids, attention_masks = tokenize_conversation(\n",
    "        conversations, tokenizer, max_length=max_length\n",
    "    )\n",
    "    emotions_labels = pad_labels(emotions, max_length=max_length)\n",
    "    triggers_labels = pad_labels(triggers, max_length=max_length)\n",
    "\n",
    "    dataset = ConversationDataset(\n",
    "        input_ids, attention_masks, emotions_labels, triggers_labels\n",
    "    )\n",
    "    loader = DataLoader(dataset, batch_size=batch_size, shuffle=shuffle)\n",
    "\n",
    "    return loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8a3-AwoX0ui4"
   },
   "outputs": [],
   "source": [
    "train_loader = create_dataloader(\n",
    "    X_train, y1_train, y2_train, \n",
    "    tokenizer=tokenizer, batch_size=batch_size, max_length=max_length, \n",
    "    shuffle=True\n",
    ")\n",
    "val_loader = create_dataloader(\n",
    "    X_val, y1_val, y2_val, \n",
    "    tokenizer=tokenizer, batch_size=batch_size, max_length=max_length, \n",
    "    shuffle=False\n",
    ")\n",
    "test_loader = create_dataloader(\n",
    "    X_test, y1_test, y2_test, \n",
    "    tokenizer=tokenizer, batch_size=batch_size, max_length=max_length, \n",
    "    shuffle=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "X5MO3g0VztW_"
   },
   "source": [
    "### **Model Definition**\n",
    "\n",
    "Defines a Mixture of Experts model with single gating mechanism, which combines expert outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kueFTpHSXViM"
   },
   "outputs": [],
   "source": [
    "gate_type = \"linear\"  # @param [\"linear\", \"mlp\"]\n",
    "expert_type = \"linear\" # @param [\"linear\", \"mlp\", \"rnn\"]\n",
    "num_experts = 2 # @param {type: \"slider\", min: 1, max: 8, step: 1}\n",
    "top_k = 2 # @param {type: \"slider\", min: 1, max: 8, step: 1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "12upnrxiXXbM"
   },
   "outputs": [],
   "source": [
    "assert top_k <= num_experts, \"Select different values for top_k and num_experts!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "umr0EC9_XuzD"
   },
   "outputs": [],
   "source": [
    "class MoEForEmotionAndTriggerClassification(nn.Module):\n",
    "    \"\"\"\n",
    "    A Mixture of Experts model for emotion and trigger classification.\n",
    "\n",
    "    This architecture uses a single gating network to aggregate predictions from \n",
    "    expert modules for two separate tasks: emotion classification and trigger detection.\n",
    "\n",
    "    Attributes:\n",
    "        model (transformers.BertModel): Pretrained BERT model for feature extraction.\n",
    "        gating_network_emotion (Union[nn.Linear, nn.Sequential]): Gating mechanism \n",
    "            for emotion classification.\n",
    "        gating_network_trigger (Union[nn.Linear, nn.Sequential]): Gating \n",
    "            mechanism for trigger classification.\n",
    "        experts (nn.ModuleList): A list of expert modules (Linear, MLP, or RNN).\n",
    "        emotion_classifier (nn.Linear): Output layer for emotion classification.\n",
    "        trigger_classifier (nn.Linear): Output layer for trigger classification.\n",
    "        k (int): Number of top experts to activate.\n",
    "        dropout (nn.Dropout): Dropout layer for regularization.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self, \n",
    "        num_experts: int, \n",
    "        k: int, \n",
    "        num_classes: int, \n",
    "        gate_type: str, \n",
    "        expert_type: str, \n",
    "        model_name: str = \"bert-base-uncased\", \n",
    "        train_bert: bool = True\n",
    "    ) -> None:\n",
    "        \"\"\"\n",
    "        Initialize the MoE model.\n",
    "\n",
    "        Args:\n",
    "            num_experts (int): Number of expert modules.\n",
    "            k (int): Number of top experts to use per forward pass.\n",
    "            num_classes (int): Number of emotion classes.\n",
    "            gate_type (str): Type of gating mechanism (\"linear\" or \"mlp\").\n",
    "            expert_type (str): Type of expert layer (\"linear\", \"mlp\", or \"rnn\").\n",
    "            model_name (str): Name of the pretrained BERT model. Defaults to \"bert-base-uncased\".\n",
    "            train_bert (bool): Whether to fine-tune the BERT model. Defaults to True.\n",
    "        \"\"\"\n",
    "        super(MoEForEmotionAndTriggerClassification, self).__init__()\n",
    "\n",
    "        self.model = BertModel.from_pretrained(model_name)\n",
    "        for param in self.model.parameters():\n",
    "            param.requires_grad = train_bert  # Set to True if you want to fine-tune model\n",
    "        \n",
    "        hidden_size = self.model.config.hidden_size\n",
    "\n",
    "        gate_setup = {\n",
    "            \"linear\": nn.Linear(hidden_size, num_experts),\n",
    "            \"mlp\": nn.Sequential(\n",
    "                nn.Linear(hidden_size, 512), \n",
    "                nn.ReLU(), \n",
    "                nn.Linear(512, num_experts)\n",
    "            ),\n",
    "        }\n",
    "\n",
    "        expert_setup = {\n",
    "            \"linear\": nn.Linear(hidden_size, hidden_size),\n",
    "            \"mlp\": nn.Sequential(\n",
    "                nn.Linear(hidden_size, 512), \n",
    "                nn.ReLU(), \n",
    "                nn.Linear(512, hidden_size)\n",
    "            ),\n",
    "            \"rnn\": nn.LSTM(hidden_size, hidden_size),\n",
    "        }\n",
    "\n",
    "        self.gating_network = gate_setup[gate_type]\n",
    "        self.experts = nn.ModuleList(\n",
    "            [expert_setup[expert_type] for _ in range(num_experts)]\n",
    "        )\n",
    "\n",
    "        self.emotion_classifier = nn.Linear(hidden_size, num_classes)\n",
    "        self.trigger_classifier = nn.Linear(hidden_size, 1)\n",
    "\n",
    "        self.k = k\n",
    "        self.dropout = nn.Dropout(p=0.1)\n",
    "\n",
    "    def forward(\n",
    "        self, \n",
    "        input_ids: torch.Tensor, \n",
    "        attention_mask: torch.Tensor\n",
    "    ) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        \"\"\"\n",
    "        Forward pass through the model.\n",
    "\n",
    "        Args:\n",
    "            input_ids (torch.Tensor): Input IDs of shape (batch_size, seq_len).\n",
    "            attention_mask (torch.Tensor): Attention mask of shape (batch_size, seq_len).\n",
    "\n",
    "        Returns:\n",
    "            Tuple[torch.Tensor, torch.Tensor]: A tuple containing:\n",
    "                - emotion_logits: Tensor of shape (batch_size, seq_len, num_classes).\n",
    "                - trigger_logits: Tensor of shape (batch_size, seq_len).\n",
    "        \"\"\"\n",
    "        model_outputs = self.model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        embeddings = model_outputs.last_hidden_state\n",
    "        pooled_embeddings = embeddings.mean(dim=1)\n",
    "        pooled_embeddings = self.dropout(pooled_embeddings)\n",
    "\n",
    "        expert_weights = torch.softmax(\n",
    "            self.gating_network(pooled_embeddings), dim=-1\n",
    "        )\n",
    "\n",
    "        combined_output = self._compute_expert_output(embeddings, expert_weights)\n",
    "        \n",
    "        combined_output = self.dropout(combined_output)\n",
    "\n",
    "        emotion_logits = self.emotion_classifier(combined_output)\n",
    "        trigger_logits = self.trigger_classifier(combined_output).squeeze(-1)\n",
    "\n",
    "        return emotion_logits, trigger_logits\n",
    "\n",
    "    def _compute_expert_output(\n",
    "        self, \n",
    "        embeddings: torch.Tensor, \n",
    "        expert_weights: torch.Tensor\n",
    "    ) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Combine outputs from the top-k selected experts.\n",
    "\n",
    "        Args:\n",
    "            embeddings (torch.Tensor): BERT embeddings of shape (batch_size, seq_len, hidden_dim).\n",
    "            expert_weights (torch.Tensor): Gating weights of shape (batch_size, num_experts).\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: Combined expert output of shape (batch_size, seq_len, hidden_dim).\n",
    "        \"\"\"\n",
    "        combined_output = torch.zeros_like(embeddings)\n",
    "        topk_weights, topk_indices = torch.topk(expert_weights, self.k, dim=-1)\n",
    "\n",
    "        for i in range(self.k):\n",
    "            expert_idx = topk_indices[:, i]\n",
    "            # Add dimensions for broadcasting across sequence length and hidden size\n",
    "            weight = topk_weights[:, i].unsqueeze(-1).unsqueeze(-1)\n",
    "\n",
    "            expert_outputs = []\n",
    "            for j in range(expert_idx.size(0)):\n",
    "                # Variant suggested for improved clarity and shape safety (ChatGPT)\n",
    "                expert = self.experts[expert_idx[j]]\n",
    "                x = embeddings[j].unsqueeze(0)\n",
    "\n",
    "                if isinstance(expert, nn.LSTM):\n",
    "                    output, _ = expert(x)\n",
    "                    expert_outputs.append(output.squeeze(0))\n",
    "                else:\n",
    "                    expert_outputs.append(expert(x.squeeze(0)))\n",
    "\n",
    "                \"\"\"\n",
    "                # TODO: Re-evaluate original version below. Retained temporarily for testing.\n",
    "                expert = self.experts[expert_idx[j]]\n",
    "\n",
    "                if isinstance(expert, nn.LSTM):\n",
    "                    embedding_input = embeddings[j].unsqueeze(0)\n",
    "                    output, _ = expert(embedding_input)\n",
    "                    expert_outputs.append(output.squeeze(0))\n",
    "\n",
    "                elif isinstance(expert, nn.Linear) or isinstance(expert, nn.Sequential):\n",
    "                    output = expert(embeddings[j])\n",
    "                    expert_outputs.append(output)\n",
    "                \"\"\"\n",
    "\n",
    "            expert_outputs = torch.stack(expert_outputs)\n",
    "            combined_output += weight * expert_outputs\n",
    "\n",
    "        return combined_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Fpyq3mUpz6D2"
   },
   "source": [
    "### **Training**\n",
    "\n",
    "Trains the MoE model and evaluates on validation set after each epoch. Logs loss and accuracy for both tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eO2uRZl2zxvO"
   },
   "outputs": [],
   "source": [
    "train_bert = True   # @param {type: \"boolean\"}\n",
    "learning_rate = 0.00002  # @param {type: \"slider\", min: 1E-5, max: 5E-5, step: 1E-5}\n",
    "num_epochs = 5  # @param {type: \"slider\", min: 3, max: 15, step: 1}\n",
    "weight_triggers = True # @param {type: \"boolean\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oJPBM0OaYWBf"
   },
   "outputs": [],
   "source": [
    "# Instantiate the Mixture of Experts model\n",
    "moe = MoEForEmotionAndTriggerClassification(\n",
    "    num_experts=num_experts, \n",
    "    k=top_k, \n",
    "    num_classes=len(labels_to_ids), \n",
    "    gate_type=gate_type, \n",
    "    expert_type=expert_type, \n",
    "    model_name=designated_model, \n",
    "    train_bert=train_bert\n",
    ")\n",
    "\n",
    "# Initialize optimizer\n",
    "optimizer = AdamW(moe.parameters(), lr=learning_rate)\n",
    "\n",
    "# Compute positive class weight for trigger classification (if applicable)\n",
    "pos_weight = None\n",
    "if weight_triggers and dataset in [\"MaSaC\", \"MaSaC_translated\"]:\n",
    "    flattened_triggers = [x for sequence in y2_train for x in sequence]\n",
    "    num_negative_samples = len([x for x in flattened_triggers if x == 0])*0.8\n",
    "    num_positive_samples = len(flattened_triggers) - num_negative_samples\n",
    "    pos_weight_value = num_negative_samples / num_positive_samples\n",
    "    pos_weight = torch.tensor([pos_weight_value], device=device)\n",
    "\n",
    "# Define loss functions\n",
    "emotion_loss_fn = CrossEntropyLoss()\n",
    "trigger_loss_fn = BCEWithLogitsLoss(pos_weight=pos_weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EiLsS9Y9YWjk"
   },
   "outputs": [],
   "source": [
    "moe.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "44x3zphNQWrq"
   },
   "outputs": [],
   "source": [
    "def remove_padding(\n",
    "    logits: torch.Tensor, \n",
    "    labels: torch.Tensor, \n",
    "    task: str\n",
    ") -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "    \"\"\"\n",
    "    Remove masked padding from logits and labels for loss computation.\n",
    "\n",
    "    This function flattens both the model outputs and target labels, then removes \n",
    "    entries where labels are set to -1 (which represent padding tokens).\n",
    "\n",
    "    Args:\n",
    "        logits (torch.Tensor): Model outputs of shape (batch_size, seq_len, num_classes)\n",
    "            for \"emotion\", or (batch_size, seq_len) for \"trigger\".\n",
    "        labels (torch.Tensor): Ground-truth labels of shape (batch_size, seq_len).\n",
    "        task (str): Classification task type, either \"emotion\" or \"trigger\".\n",
    "\n",
    "    Returns:\n",
    "        Tuple[torch.Tensor, torch.Tensor]: A tuple containing a pair of 1d tensors:\n",
    "            - logits: Flattened logits excluding padding.\n",
    "            - labels: Flattened ground-truth labels excluding padding.\n",
    "    \"\"\"\n",
    "    assert task in {\"emotion\", \"trigger\"}, \"task must be 'emotion' or 'trigger'\"\n",
    "\n",
    "    valid_positions = labels != -1\n",
    "\n",
    "    logits_flat = (\n",
    "        logits.view(-1, logits.size(-1)) \n",
    "        if task == \"emotion\" \n",
    "        else logits.view(-1)\n",
    "    )\n",
    "    labels_flat = labels.view(-1)\n",
    "\n",
    "    logits = logits_flat[valid_positions.view(-1)]\n",
    "    labels = labels_flat[valid_positions.view(-1)]\n",
    "\n",
    "    return logits, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7QS5wNkiYZt7"
   },
   "outputs": [],
   "source": [
    "def evaluate(\n",
    "    model: MoEForEmotionAndTriggerClassification, \n",
    "    val_loader: DataLoader, \n",
    "    device: torch.device, \n",
    "    emotion_loss_fn: Callable[[torch.Tensor, torch.Tensor], torch.Tensor], \n",
    "    trigger_loss_fn: Callable[[torch.Tensor, torch.Tensor], torch.Tensor], \n",
    "    verbose: bool = True\n",
    ") -> Tuple[float, float, List[str]]:\n",
    "    \"\"\"\n",
    "    Evaluates the Mixture-of-Experts model on a validation dataset.\n",
    "\n",
    "    Computes total loss, emotion classification accuracy, and trigger classification\n",
    "    accuracy. Logs validation loss every 100 steps.\n",
    "\n",
    "    Args:\n",
    "        model (MoEForEmotionAndTriggerClassification): The trained model to evaluate.\n",
    "        val_loader (DataLoader): Dataloader for the validation set.\n",
    "        device (torch.device): Represents the device on which a torch.Tensor is \n",
    "            or will be allocated.\n",
    "        emotion_loss_fn (Callable[[torch.Tensor, torch.Tensor], torch.Tensor]): \n",
    "            Loss function used for emotion classification (e.g., CrossEntropyLoss).\n",
    "        trigger_loss_fn (Callable[[torch.Tensor, torch.Tensor], torch.Tensor]): \n",
    "            Loss function used for trigger classification (e.g., BCEWithLogitsLoss).\n",
    "        verbose (bool): Controls the verbosity. Set False to hide messages. \n",
    "            Defaults to True.\n",
    "\n",
    "    Returns:\n",
    "        Tuple[float, float, List[str]]: A tuple containing:\n",
    "            - avg_val_loss: Mean validation loss across all batches.\n",
    "            - avg_val_accuracy: Mean of emotion and trigger accuracy.\n",
    "            - val_logs: Log messages recorded during validation.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    val_loss, nb_steps = 0.0, 0\n",
    "    total_emotion_preds, correct_emotion_preds = 0, 0\n",
    "    total_trigger_preds, correct_trigger_preds = 0, 0\n",
    "    val_logs = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for idx, batch in enumerate(val_loader):\n",
    "            # forward pass\n",
    "            input_ids = batch[\"input_ids\"].to(device)\n",
    "            attention_mask = batch[\"attention_mask\"].to(device)\n",
    "            emotion_labels = batch[\"emotion_labels\"].to(device)\n",
    "            trigger_labels = batch[\"trigger_labels\"].to(device)\n",
    "\n",
    "            emotion_logits, trigger_logits = model(input_ids, attention_mask)\n",
    "\n",
    "            # remove padding\n",
    "            emotion_logits, emotion_labels = remove_padding(\n",
    "                emotion_logits, emotion_labels, \"emotion\"\n",
    "            )\n",
    "            trigger_logits, trigger_labels = remove_padding(\n",
    "                trigger_logits, trigger_labels, \"trigger\"\n",
    "            )\n",
    "\n",
    "            # compute loss\n",
    "            emotion_loss = emotion_loss_fn(emotion_logits, emotion_labels.long())\n",
    "            trigger_loss = trigger_loss_fn(trigger_logits, trigger_labels)\n",
    "\n",
    "            loss = emotion_loss + trigger_loss\n",
    "            val_loss += loss.item()\n",
    "\n",
    "            # compute accuracy\n",
    "            emotion_preds = torch.argmax(emotion_logits, dim=-1)\n",
    "            trigger_preds = (torch.sigmoid(trigger_logits).squeeze(-1) > 0.5).long()\n",
    "\n",
    "            correct_emotion_preds += torch.sum(emotion_preds == emotion_labels).item()\n",
    "            correct_trigger_preds += torch.sum(trigger_preds == trigger_labels).item()\n",
    "\n",
    "            total_emotion_preds += emotion_labels.numel()\n",
    "            total_trigger_preds += trigger_labels.numel()\n",
    "\n",
    "            nb_steps += 1\n",
    "\n",
    "            # logging\n",
    "            if verbose and idx % 100 == 0:\n",
    "                loss_step = val_loss / nb_steps\n",
    "                print(f\"      Validation loss per 100 training steps: {loss_step:.4f}\")\n",
    "                val_logs.append(f\"      Validation loss per 100 training steps: {loss_step:.4f}\\n\")\n",
    "\n",
    "        avg_val_loss = val_loss / max(len(val_loader), 1)\n",
    "        emotion_accuracy = correct_emotion_preds / max(total_emotion_preds, 1)\n",
    "        trigger_accuracy = correct_trigger_preds / max(total_trigger_preds, 1)\n",
    "        avg_val_accuracy = (emotion_accuracy + trigger_accuracy)/2\n",
    "\n",
    "    return avg_val_loss, avg_val_accuracy, val_logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6Cq8fwysYaY_"
   },
   "outputs": [],
   "source": [
    "def train_and_validate(\n",
    "    model: MoEForEmotionAndTriggerClassification, \n",
    "    train_loader: DataLoader, \n",
    "    val_loader: DataLoader, \n",
    "    optimizer: torch.optim.Optimizer, \n",
    "    device: torch.device, \n",
    "    emotion_loss_fn: Callable[[torch.Tensor, torch.Tensor], torch.Tensor], \n",
    "    trigger_loss_fn: Callable[[torch.Tensor, torch.Tensor], torch.Tensor], \n",
    "    num_epochs: int = 3, \n",
    "    verbose: bool = True\n",
    ") -> List[str]:\n",
    "    \"\"\"\n",
    "    Train and evaluate the Mixture-of-Experts model for emotion and trigger classification.\n",
    "\n",
    "    For each epoch, computes training loss and accuracy for both tasks. Logs \n",
    "    training loss every 100 steps and evaluates the model on the validation set \n",
    "    at the end of each epoch.\n",
    "\n",
    "    Args:\n",
    "        model (MoEForEmotionAndTriggerClassification): The model to train and evaluate.\n",
    "        train_loader (DataLoader): Dataloader for the train set.\n",
    "        val_loader (DataLoader): Dataloader for the validation set.\n",
    "        optimizer (torch.optim.Optimizer): PyTorch optimizer.\n",
    "        device (torch.device): Represents the device on which a torch.Tensor is \n",
    "            or will be allocated.\n",
    "        emotion_loss_fn (Callable[[torch.Tensor, torch.Tensor], torch.Tensor]): \n",
    "            Loss function used for emotion classification (e.g., CrossEntropyLoss).\n",
    "        trigger_loss_fn (Callable[[torch.Tensor, torch.Tensor], torch.Tensor]): \n",
    "            Loss function used for trigger classification (e.g., BCEWithLogitsLoss).\n",
    "        num_epochs (int, optional): Number of training epochs. Defaults to 3.\n",
    "        verbose (bool): Controls the verbosity. Set False to hide messages. \n",
    "            Defaults to True.\n",
    "\n",
    "    Returns:\n",
    "        List[str]: A list of formatted strings logging training and validation \n",
    "        progress for each epoch and key steps within.\n",
    "    \"\"\"\n",
    "    train_logs = []\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        print(f\"\\nEpoch [{epoch + 1}/{num_epochs}]\")\n",
    "        train_logs.append(f\"Epoch [{epoch + 1}/{num_epochs}]\\n\")\n",
    "        model.train()\n",
    "\n",
    "        train_loss, nb_steps = 0.0, 0\n",
    "        total_emotion_preds, correct_emotion_preds = 0, 0\n",
    "        total_trigger_preds, correct_trigger_preds = 0, 0\n",
    "\n",
    "        for idx, batch in enumerate(train_loader):\n",
    "            # forward pass\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            input_ids = batch[\"input_ids\"].to(device)\n",
    "            attention_mask = batch[\"attention_mask\"].to(device)\n",
    "            emotion_labels = batch[\"emotion_labels\"].to(device)\n",
    "            trigger_labels = batch[\"trigger_labels\"].to(device)\n",
    "\n",
    "            emotion_logits, trigger_logits = model(input_ids, attention_mask)\n",
    "\n",
    "            # remove padding\n",
    "            emotion_logits, emotion_labels = remove_padding(\n",
    "                emotion_logits, emotion_labels, \"emotion\"\n",
    "            )\n",
    "            trigger_logits, trigger_labels = remove_padding(\n",
    "                trigger_logits, trigger_labels, \"trigger\"\n",
    "            )\n",
    "\n",
    "            # compute loss\n",
    "            emotion_loss = emotion_loss_fn(emotion_logits, emotion_labels.long())\n",
    "            trigger_loss = trigger_loss_fn(trigger_logits, trigger_labels)\n",
    "\n",
    "            loss = emotion_loss + trigger_loss\n",
    "            train_loss += loss.item()\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # compute accuracy\n",
    "            emotion_preds = torch.argmax(emotion_logits, dim=-1)\n",
    "            trigger_preds = (torch.sigmoid(trigger_logits).squeeze(-1) > 0.5).long()\n",
    "\n",
    "            correct_emotion_preds += torch.sum(emotion_preds == emotion_labels).item()\n",
    "            correct_trigger_preds += torch.sum(trigger_preds == trigger_labels).item()\n",
    "\n",
    "            total_emotion_preds += emotion_labels.numel()\n",
    "            total_trigger_preds += trigger_labels.numel()\n",
    "\n",
    "            nb_steps += 1\n",
    "\n",
    "            # logging\n",
    "            if verbose and idx % 100 == 0:\n",
    "                loss_step = train_loss / nb_steps\n",
    "                print(f\"      Training loss per 100 training steps: {loss_step:.4f}\")\n",
    "                train_logs.append(f\"      Training loss per 100 training steps: {loss_step:.4f}\\n\")\n",
    "\n",
    "        avg_train_loss = train_loss / max(len(train_loader), 1)\n",
    "        emotion_accuracy = correct_emotion_preds / max(total_emotion_preds, 1)\n",
    "        trigger_accuracy = correct_trigger_preds / max(total_trigger_preds, 1)\n",
    "        avg_train_accuracy = (emotion_accuracy + trigger_accuracy)/2\n",
    "\n",
    "        val_loss, val_accuracy, val_logs = evaluate(\n",
    "            model, \n",
    "            val_loader, \n",
    "            device=device, \n",
    "            emotion_loss_fn=emotion_loss_fn, \n",
    "            trigger_loss_fn=trigger_loss_fn, \n",
    "            verbose=verbose\n",
    "        )\n",
    "        train_logs.extend(val_logs)\n",
    "        train_logs.append(f\"   Training Loss: {avg_train_loss:.4f}, Training Accuracy: {avg_train_accuracy:.4f}\\n\")\n",
    "        train_logs.append(f\"   Validation Loss: {val_loss:.4f}, Validation Accuracy: {val_accuracy:.4f}\\n\\n\")\n",
    "\n",
    "        if verbose:\n",
    "            print(f\"   Training Loss: {avg_train_loss:.4f}, Training Accuracy: {avg_train_accuracy:.4f}\")\n",
    "            print(f\"   Validation Loss: {val_loss:.4f}, Validation Accuracy: {val_accuracy:.4f}\\n\")\n",
    "\n",
    "    return train_logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "N_U28Fs-YcFL"
   },
   "outputs": [],
   "source": [
    "logs = train_and_validate(\n",
    "    moe, \n",
    "    train_loader, \n",
    "    val_loader, \n",
    "    optimizer=optimizer, \n",
    "    device=device, \n",
    "    emotion_loss_fn=emotion_loss_fn, \n",
    "    trigger_loss_fn=trigger_loss_fn, \n",
    "    num_epochs=num_epochs, \n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Evaluation**\n",
    "\n",
    "Runs on test set to calculate accuracy, precision, recall, and F1 for emotion and trigger tasks. Also generates confusion matrices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2To3TjUdUEsR"
   },
   "outputs": [],
   "source": [
    "def get_metrics(\n",
    "    model: MoEForEmotionAndTriggerClassification, \n",
    "    data_loader: DataLoader, \n",
    "    device: torch.device, \n",
    "    labels_to_ids: Dict[str, int], \n",
    "    ids_to_labels: Dict[int, str]\n",
    ") -> Tuple[Dict[str, Any], np.ndarray, np.ndarray]:\n",
    "    \"\"\"\n",
    "    Evaluate a trained Mixture-of-Experts model on a test set and compute metrics.\n",
    "\n",
    "    Calculates accuracy, precision, recall, and F1 scores for both emotion and\n",
    "    trigger classification tasks, as well as confusion matrices.\n",
    "\n",
    "    Args:\n",
    "        model (MoEForEmotionAndTriggerClassification): The trained model.\n",
    "        data_loader (DataLoader): DataLoader for the test set.\n",
    "        device (torch.device): The device to use for computation.\n",
    "        labels_to_ids (Dict[str, int]): Dictionary mapping strings into integers.\n",
    "        ids_to_labels (Dict[int, str]): Dictionary mapping integers to labels.\n",
    "\n",
    "    Returns:\n",
    "        Tuple[Dict[str, Any], np.ndarray, np.ndarray]: A tuple containing:\n",
    "            - metrics (dict): Dictionary with aggregated performance metrics.\n",
    "            - emotion_cm (np.ndarray): Confusion matrix for emotion classification.\n",
    "            - trigger_cm (np.ndarray): Confusion matrix for trigger classification.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "\n",
    "    # Initialize metrics\n",
    "    emotion_accuracy, emotion_precision, emotion_recall, emotion_f1 = 0.0, 0.0, 0.0, 0.0\n",
    "    trigger_accuracy, trigger_precision, trigger_recall, trigger_f1 = 0.0, 0.0, 0.0, 0.0\n",
    "\n",
    "    emotion_cm = None\n",
    "    trigger_cm = None\n",
    "\n",
    "    unique_emotions_f1 = dict.fromkeys(labels_to_ids.keys(), 0.0)\n",
    "    nb_steps = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in data_loader:\n",
    "            input_ids = batch[\"input_ids\"].to(device)\n",
    "            attention_mask = batch[\"attention_mask\"].to(device)\n",
    "            emotion_labels = batch[\"emotion_labels\"].to(device)\n",
    "            trigger_labels = batch[\"trigger_labels\"].to(device)\n",
    "\n",
    "            emotion_logits, trigger_logits = model(input_ids, attention_mask)\n",
    "\n",
    "            # Emotion classification\n",
    "            emotion_logits, emotion_labels = remove_padding(\n",
    "                emotion_logits, emotion_labels, \"emotion\"\n",
    "            )\n",
    "\n",
    "            emotion_preds = torch.argmax(emotion_logits, dim=-1)\n",
    "\n",
    "            emotion_preds_np = emotion_preds.cpu().numpy()\n",
    "            emotion_labels_np = emotion_labels.cpu().numpy()\n",
    "\n",
    "            emotion_accuracy += accuracy_score(emotion_labels_np, emotion_preds_np)\n",
    "            emotion_precision += precision_score(\n",
    "                emotion_labels_np, emotion_preds_np, average=\"weighted\", zero_division=0\n",
    "            )\n",
    "            emotion_recall += recall_score(\n",
    "                emotion_labels_np, emotion_preds_np, average=\"weighted\", zero_division=0\n",
    "            )\n",
    "            emotion_f1 += f1_score(\n",
    "                emotion_labels_np, emotion_preds_np, average=\"weighted\", zero_division=0\n",
    "            )\n",
    "\n",
    "            for idx, score in enumerate(\n",
    "                f1_score(emotion_labels_np, emotion_preds_np, average=None, zero_division=0)\n",
    "            ):\n",
    "                unique_emotions_f1[ids_to_labels[idx]] += score\n",
    "\n",
    "            emotion_cm_batch = confusion_matrix(\n",
    "                emotion_labels_np, emotion_preds_np, labels=list(range(len(labels_to_ids)))\n",
    "            )\n",
    "            emotion_cm = (\n",
    "                emotion_cm_batch \n",
    "                if emotion_cm is None \n",
    "                else emotion_cm + emotion_cm_batch\n",
    "            )\n",
    "\n",
    "            # Trigger classification\n",
    "            trigger_logits, trigger_labels = remove_padding(\n",
    "                trigger_logits, trigger_labels, \"trigger\"\n",
    "            )\n",
    "\n",
    "            trigger_preds = (torch.sigmoid(trigger_logits).squeeze(-1) > 0.5).long()\n",
    "\n",
    "            trigger_preds_np = trigger_preds.cpu().numpy()\n",
    "            trigger_labels_np = trigger_labels.cpu().numpy()\n",
    "\n",
    "            trigger_accuracy += accuracy_score(trigger_labels_np, trigger_preds_np)\n",
    "            trigger_precision += precision_score(\n",
    "                trigger_labels_np, trigger_preds_np, average=\"weighted\", zero_division=0\n",
    "            )\n",
    "            trigger_recall += recall_score(\n",
    "                trigger_labels_np, trigger_preds_np, average=\"weighted\", zero_division=0\n",
    "            )\n",
    "            trigger_f1 += f1_score(\n",
    "                trigger_labels_np, trigger_preds_np, average=\"weighted\", zero_division=0\n",
    "            )\n",
    "\n",
    "            trigger_cm_batch = confusion_matrix(\n",
    "                trigger_labels_np, trigger_preds_np, labels=[0, 1]\n",
    "            )\n",
    "            trigger_cm = (\n",
    "                trigger_cm_batch \n",
    "                if trigger_cm is None \n",
    "                else trigger_cm + trigger_cm_batch\n",
    "            )\n",
    "\n",
    "            nb_steps += 1\n",
    "\n",
    "    # Aggregate metrics\n",
    "    metrics = defaultdict(lambda: {})\n",
    "    nb_steps = max(nb_steps, 1)\n",
    "\n",
    "    metrics[\"emotion_classification\"] = {\n",
    "        \"accuracy\": emotion_accuracy / nb_steps, \n",
    "        \"precision\": emotion_precision / nb_steps, \n",
    "        \"recall\": emotion_recall / nb_steps, \n",
    "        \"f1\": {\n",
    "            \"avg\": emotion_f1 / nb_steps\n",
    "        },\n",
    "    }\n",
    "\n",
    "    for key in unique_emotions_f1:\n",
    "        metrics[\"emotion_classification\"][\"f1\"][key] = unique_emotions_f1[key] / nb_steps\n",
    "    \n",
    "    metrics[\"trigger_classification\"] = {\n",
    "        \"accuracy\": trigger_accuracy / nb_steps, \n",
    "        \"precision\": trigger_precision / nb_steps, \n",
    "        \"recall\": trigger_recall / nb_steps, \n",
    "        \"f1\": trigger_f1 / nb_steps,\n",
    "    }\n",
    "\n",
    "    return metrics, emotion_cm, trigger_cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "m2819G500ui7"
   },
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(\n",
    "    cm: np.ndarray, \n",
    "    labels: List[str], \n",
    "    title: str = \"Confusion Matrix\",\n",
    "    save: bool = False\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    Plot a labeled confusion matrix using Seaborn heatmap.\n",
    "\n",
    "    Args:\n",
    "        cm (np.ndarray): Confusion matrix of shape (n_classes, n_classes).\n",
    "        labels (List[str]): Class labels to use on axes.\n",
    "        title (str, optional): Title of the plot. Defaults to \"Confusion Matrix\".\n",
    "        save (bool, optional): If True, saves the plot to disk. Defaults to False.\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    sns.heatmap(\n",
    "        cm, \n",
    "        annot=True, \n",
    "        fmt=\"d\", \n",
    "        cmap=\"Blues\", \n",
    "        xticklabels=labels, \n",
    "        yticklabels=labels, \n",
    "        cbar=False\n",
    "    )\n",
    "    plt.xlabel(\"Predicted Labels\", fontsize=14)\n",
    "    plt.ylabel(\"True Labels\", fontsize=14)\n",
    "    plt.title(title, fontsize=16)\n",
    "    plt.xticks(rotation=45, ha=\"right\")\n",
    "    plt.yticks(rotation=0)\n",
    "    plt.tight_layout()\n",
    "\n",
    "    if save:\n",
    "        filename = title.lower().replace(\" \", \"_\") + \".png\"\n",
    "        plt.savefig(filename, dpi=300)\n",
    "        \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8UNspSJxUgLJ"
   },
   "outputs": [],
   "source": [
    "metrics, emotion_cm, trigger_cm = get_metrics(\n",
    "    moe, \n",
    "    test_loader, \n",
    "    device, \n",
    "    labels_to_ids, \n",
    "    ids_to_labels\n",
    ")\n",
    "\n",
    "# Output results\n",
    "for task, results in metrics.items():\n",
    "    print(f\"Task: {task.upper()}\")\n",
    "    for metric, score in results.items():\n",
    "        if metric == \"f1\" and isinstance(score, dict):\n",
    "            print(f\"      f1: \")\n",
    "            max_label_len = max(len(str(k)) for k in score.keys())\n",
    "            for x, y in score.items():\n",
    "                print(f\"          {x:<{max_label_len}}: {y:.4f}\")\n",
    "        else:\n",
    "            print(f\"      {metric}: {score:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vu-QJEmJ0ui7"
   },
   "outputs": [],
   "source": [
    "plot_confusion_matrix(\n",
    "    emotion_cm, \n",
    "    list(labels_to_ids), \n",
    "    \"Emotion Classification\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "85ShzciV0ui7"
   },
   "outputs": [],
   "source": [
    "plot_confusion_matrix(\n",
    "    trigger_cm, \n",
    "    [\"No trigger\", \"Trigger\"], \n",
    "    \"Trigger Classification\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qiwgK0H60ui8"
   },
   "source": [
    "### **Save Experiment**\n",
    "\n",
    "Saves experiment configuration, performance metrics, and confusion matrices to the `results/<dataset>` folder. To save state_dict of the trained model, uncomment the cell below and the state_dict will be saved to `trained_model/<dataset>` folder."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Output Format\n",
    "\n",
    "Each experiment result is saved to a `.txt` file in the `results/<dataset>/` folder.\n",
    "\n",
    "Each file includes:\n",
    "- Experiment config summary (batch size, epochs, gating type, etc.)\n",
    "- Training and validation loss/accuracy per epoch\n",
    "- Final test set metrics (accuracy, precision, recall, F1)\n",
    "- Per-class F1 scores for emotion labels\n",
    "- Confusion matrices for emotion and trigger classification\n",
    "\n",
    "Example file path:\n",
    "\n",
    "`results/MELD/moe_model_True_train_bert_linear_single_gate_4_mlp_experts_4_active_3e-5_lr_10_epochs.txt`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "s7hH0xNVYdqL"
   },
   "outputs": [],
   "source": [
    "# # Optional: Save the trained model\n",
    "# # --------------------------------\n",
    "# # Uncomment this section to save the trained model to disk.\n",
    "\n",
    "# model_dir = os.path.join(\"trained_models\", dataset)\n",
    "# os.makedirs(model_dir, exist_ok=True)\n",
    "\n",
    "# model_name = (\n",
    "#     f\"moe_model_{train_bert}_train_bert_{gate_type}_single_gate_\"\n",
    "#     f\"{num_experts}_{expert_type}_experts_{top_k}_active_\"\n",
    "#     f\"{learning_rate}_lr_{num_epochs}_epochs.pth\"\n",
    "# )\n",
    "# model_path = os.path.join(model_dir, model_name)\n",
    "\n",
    "# torch.save(moe.state_dict(), model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wwX2HcS0vQQr"
   },
   "outputs": [],
   "source": [
    "def write_confusion_matrix(\n",
    "    title: str, \n",
    "    cm: np.ndarray, \n",
    "    labels: List[str]\n",
    ") -> List[str]:\n",
    "    \"\"\"\n",
    "    Format a confusion matrix as a list of strings for saving to a .txt file.\n",
    "\n",
    "    Args:\n",
    "        title (str): Title to be displayed above the matrix.\n",
    "        cm (np.ndarray): Confusion matrix of shape (n_classes, n_classes).\n",
    "        labels (List[str]): Class labels corresponding to matrix rows/columns.\n",
    "\n",
    "    Returns:\n",
    "        List[str]: A list of strings representing the formatted confusion matrix,\n",
    "            ready to be written to a plain text file.\n",
    "    \"\"\"\n",
    "    cm2txt = []\n",
    "    col_width = max(len(str(label)) for label in labels) + 1\n",
    "\n",
    "    cm2txt.append(f\"\\n{title}\\n\")\n",
    "\n",
    "    # Header row\n",
    "    header = f\"{'':<{col_width}}\" + \"\".join(\n",
    "        f\"{label:<{col_width}}\" for label in labels\n",
    "    )\n",
    "    cm2txt.append(header + \"\\n\")\n",
    "\n",
    "    # Matrix rows\n",
    "    for i, label in enumerate(labels):\n",
    "        row = f\"{label:<{col_width}}\" + \"\".join(\n",
    "            f\"{value:<{col_width}}\" for value in cm[i]\n",
    "        )\n",
    "        cm2txt.append(row + \"\\n\")\n",
    "\n",
    "    return cm2txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cR5h2JI40ui8"
   },
   "outputs": [],
   "source": [
    "# Construct file path\n",
    "model_name = (\n",
    "    f\"weighted_moe_model\" if weight_triggers and dataset == \"MaSaC\"\n",
    "    else \"moe_model\"\n",
    ")\n",
    "file_name = (\n",
    "    f\"{model_name}_{train_bert}_train_bert_{gate_type}_single_gate_\"\n",
    "    f\"{num_experts}_{expert_type}_experts_{top_k}_active_\"\n",
    "    f\"{learning_rate}_lr_{num_epochs}_epochs.txt\"\n",
    ")\n",
    "file_path = os.path.join(\"results\", dataset, file_name)\n",
    "\n",
    "# Ensure the directory exists\n",
    "os.makedirs(os.path.dirname(file_path), exist_ok=True)\n",
    "\n",
    "with open(file_path, \"w\") as f:\n",
    "    # Experiment setup metadata\n",
    "    experiment_setup = [\n",
    "        f\"max_length = {max_length}\\n\",\n",
    "        f\"batch_size = {batch_size}\\n\\n\",\n",
    "        f\"gate_type = {gate_type}\\n\",\n",
    "        f\"expert_type = {expert_type}\\n\",\n",
    "        f\"num_experts = {num_experts}\\n\",\n",
    "        f\"top_k = {top_k}\\n\",\n",
    "        f\"learning_rate = {learning_rate}\\n\",\n",
    "        f\"num_epochs = {num_epochs}\\n\",\n",
    "        f\"train_bert = {train_bert}\\n\",\n",
    "        f\"weight_triggers = {weight_triggers}\\n\\n\"\n",
    "    ]\n",
    "    f.writelines(experiment_setup)\n",
    "\n",
    "    # Training logs\n",
    "    f.writelines(logs)\n",
    "    f.write(\"\\n\")\n",
    "\n",
    "    # Final metrics\n",
    "    experiment_results = []\n",
    "    for task, results in metrics.items():\n",
    "        experiment_results.append(f\"\\nTask: {task.upper()}\\n\")\n",
    "        for metric, score in results.items():\n",
    "            if metric == \"f1\" and isinstance(score, dict):\n",
    "                experiment_results.append(\"      f1:\\n\")\n",
    "                max_label_len = max(len(str(k)) for k in score.keys())\n",
    "                for x, y in score.items():\n",
    "                    experiment_results.append(f\"          {x:<{max_label_len}}: {y:.4f}\\n\")\n",
    "            else:\n",
    "                experiment_results.append(f\"      {metric}: {score:.4f}\\n\")\n",
    "    f.writelines(experiment_results)\n",
    "\n",
    "    # Confusion matrices\n",
    "    writeable_emotion_cm = write_confusion_matrix(\n",
    "        \"Emotion confusion matrix\", emotion_cm, [i for i in labels_to_ids.keys()]\n",
    "    )\n",
    "    f.writelines(writeable_emotion_cm)\n",
    "    \n",
    "    f.write(\"\\n\")\n",
    "    writeable_trigger_cm = write_confusion_matrix(\n",
    "        \"Trigger confusion matrix\", trigger_cm, [\"No trigger\", \"Trigger\"]\n",
    "    )\n",
    "    f.writelines(writeable_trigger_cm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "goQBmT0AUx-x"
   },
   "source": [
    "### Load and test trained model\n",
    "\n",
    "Loads state_dict from `trained_models/<dataset>/` folder, runs inference on test set, calculates metrics and plots confusion matrices for both tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Y8SZCnMSYfQU"
   },
   "outputs": [],
   "source": [
    "# # Instantiate the Mixture of Experts model\n",
    "# moe_loaded = MoEForEmotionAndTriggerClassification(\n",
    "#     num_experts=num_experts,\n",
    "#     k=top_k,\n",
    "#     num_classes=len(labels_to_ids),\n",
    "#     gate_type=gate_type,\n",
    "#     expert_type=expert_type,\n",
    "#     model_name=designated_model,\n",
    "#     train_bert=train_bert,\n",
    "# )\n",
    "\n",
    "# # Load state dict from file (onto CPU in this case)\n",
    "# model_path = (\n",
    "#     f\"trained_models/{dataset}/moe_model_{train_bert}_train_bert_\"\n",
    "#     f\"{gate_type}_single_gate_{num_experts}_{expert_type}_experts_\"\n",
    "#     f\"{top_k}_active_{learning_rate}_lr_{num_epochs}_epochs.pth\"\n",
    "# )\n",
    "# moe_loaded.load_state_dict(torch.load(model_path, map_location=torch.device(\"cpu\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lrsLrQjgUxeB"
   },
   "outputs": [],
   "source": [
    "# metrics, emotion_cm, trigger_cm = get_metrics(\n",
    "#     moe_loaded, \n",
    "#     test_loader, \n",
    "#     torch.device(\"cpu\"), \n",
    "#     labels_to_ids, \n",
    "#     ids_to_labels\n",
    "# )\n",
    "\n",
    "# # Output results\n",
    "# for task, results in metrics.items():\n",
    "#     print(f\"Task: {task.upper()}\")\n",
    "#     for metric, score in results.items():\n",
    "#         if metric == \"f1\" and isinstance(score, dict):\n",
    "#             print(f\"      f1: \")\n",
    "#             max_label_len = max(len(str(k)) for k in score.keys())\n",
    "#             for x, y in score.items():\n",
    "#                 print(f\"          {x:<{max_label_len}}: {y:.4f}\")\n",
    "#         else:\n",
    "#             print(f\"      {metric}: {score:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "O3_2OzBi0ui9"
   },
   "outputs": [],
   "source": [
    "# plot_confusion_matrix(\n",
    "#     emotion_cm, \n",
    "#     list(labels_to_ids), \n",
    "#     \"Emotion Classification\"\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mQU_IDQ90ui9"
   },
   "outputs": [],
   "source": [
    "# plot_confusion_matrix(\n",
    "#     trigger_cm, \n",
    "#     [\"No trigger\", \"Trigger\"], \n",
    "#     \"Trigger Classification\"\n",
    "#     )"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
