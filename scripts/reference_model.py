# -*- coding: utf-8 -*-
"""mixture_of_experts_reference_model.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1msRdzHVUbcF1jHCaYhmsuXNBkMOU4Mhn
"""

import torch
import torch.nn as nn
from torch.utils.data import Dataset, DataLoader
from transformers import BertTokenizer, BertModel
import pandas as pd
import numpy as np

import argparse

def parse_arguments():
    parser = argparse.ArgumentParser(description="Run experiments for emotion and trigger classification.")

    parser.add_argument("DATASET", type=str, help="The dataset to use ('MELD', 'MaSaC', 'MaSaC_translated').")
    parser.add_argument("--WEIGHT_TRIGGERS", type=bool, default=False, nargs='?', const=True,
                        help="Whether to apply weight to trigger classification. Pass flag to apply weighting."
                        )
    parser.add_argument("--TRAIN_BERT", type=bool, default=False, nargs='?', const=True,
                        help="Whether to set BERT params as trainable. Pass flag to set params to trainable."
                        )
    parser.add_argument('CONFIG_PATH', type=str, help="Path to the experiment configuration YAML file.")

    return parser.parse_args()

if __name__ == '__main__':
    args = parse_arguments()
    DATASET = args.DATASET
    WEIGHT_TRIGGERS = args.WEIGHT_TRIGGERS
    TRAIN_BERT = args.TRAIN_BERT
    CONFIG_PATH = args.CONFIG_PATH

    from torch import cuda
    device = 'cuda' if cuda.is_available() else 'cpu'

    # Get experiment configurations
    import yaml

    with open(CONFIG_PATH, 'r') as file:
        experiments = yaml.safe_load(file)

    for experiment_id, params in experiments.items():
        print(f'Starting experiment number {experiment_id}.')
        LEARNING_RATE = params['LEARNING_RATE']
        NUM_EPOCHS = params['NUM_EPOCHS']

        """# Data preparation"""

        MAX_LENGTH = 128    # @param [96, 128, 256] {type: 'raw'}
        BATCH_SIZE = 32    # @param [8, 16, 32] {type: 'raw'}

        if DATASET in ['MELD', 'MaSaC_translated']:
            DESIGNATED_MODEL = 'bert-base-cased'
        elif DATASET == 'MaSaC':
            DESIGNATED_MODEL = 'bert-base-multilingual-cased'
        else:
            print(f"{DATASET} dataset is not supported.")
            exit()

        def get_data(dataset_name, stage):
            def to_float(x):
                try:
                    return float(x)
                except ValueError:
                    return 1

            df = pd.read_json(f'data/EDiReF_{stage}_data/{dataset_name}_{stage}_efr.json')
            df["triggers"] = df["triggers"].apply(lambda lst: [np.nan if x is None else x for x in lst])
            df = df[df["triggers"].apply(lambda lst: not any(pd.isna(x) for x in lst))]
            df["triggers"] = df["triggers"].apply(lambda lst: [to_float(x) for x in lst])

            conversations = list(df['utterances'])
            emotions = list(df['emotions'])
            triggers = list(df['triggers'])

            return conversations, emotions, triggers

        train_conversations, train_emotions, train_triggers = get_data(DATASET, 'train')
        val_conversations, val_emotions, val_triggers = get_data(DATASET, 'val')

        conversations = train_conversations + val_conversations
        emotions = train_emotions + val_emotions
        triggers = train_triggers + val_triggers

        flattened_emotions = [sent for conv in emotions for sent in conv]
        unique_emotions = set(flattened_emotions)

        labels_to_ids = {k: v for v, k in enumerate(unique_emotions)}
        ids_to_labels = {v: k for v, k in enumerate(unique_emotions)}
        emotions = [[labels_to_ids[emotion] for emotion in conv] for conv in emotions]

        from sklearn.model_selection import train_test_split

        def train_val_test_split(X, y1, y2, val_size = 0.2, test_size = 0.2, random_state = None):
            X_train_val, X_test, y1_train_val, y1_test, y2_train_val, y2_test = train_test_split(
                X, y1, y2, test_size=test_size, random_state=random_state
            )

            val_relative_size = val_size / (1 - test_size)

            X_train, X_val, y1_train, y1_val, y2_train, y2_val = train_test_split(
                X_train_val, y1_train_val, y2_train_val, test_size=val_relative_size, random_state=random_state
            )

            return (X_train, X_val, X_test, y1_train, y1_val, y1_test, y2_train, y2_val, y2_test)

        X_train, X_val, X_test, y1_train, y1_val, y1_test, y2_train, y2_val, y2_test = train_val_test_split(
            conversations, emotions, triggers, test_size=0.15, val_size=0.15, random_state=2024
            )

        tokenizer = BertTokenizer.from_pretrained(DESIGNATED_MODEL)

        def tokenize_conversation(conversations, max_length = 128):
            input_ids = []
            attention_masks = []

            for conversation in conversations:
                dialogue = " [SEP] ".join(conversation)
                encoded = tokenizer(
                    dialogue,
                    truncation = True,
                    padding = 'max_length',
                    max_length = max_length,
                    return_tensors = "pt"
                )
                input_ids.append(encoded["input_ids"].squeeze(0))
                attention_masks.append(encoded["attention_mask"].squeeze(0))

            return input_ids, attention_masks

        def pad_labels(labels, max_length = 128):
            padded_labels = []
            for label_set in labels:
                label_tensor = torch.tensor(label_set, dtype = torch.float)
                # Pad with -1 to ignore padding tokens in the loss function
                padded_tensor = torch.cat(
                    [label_tensor, torch.full((max_length - len(label_set),), -1)]
                )
                padded_labels.append(padded_tensor)
            return padded_labels

        class ConversationDataset(Dataset):
            def __init__(self, input_ids, attention_masks, emotion_labels, trigger_labels):
                self.input_ids = input_ids
                self.attention_masks = attention_masks
                self.emotion_labels = emotion_labels
                self.trigger_labels = trigger_labels

            def __len__(self):
                return len(self.input_ids)

            def __getitem__(self, idx):
                return {
                    "input_ids": self.input_ids[idx],
                    "attention_mask": self.attention_masks[idx],
                    "emotion_labels": self.emotion_labels[idx],
                    "trigger_labels": self.trigger_labels[idx],
                }

        def create_dataloader(conversations, emotions, triggers, max_length = 128):
            input_ids, attention_masks = tokenize_conversation(conversations, max_length = max_length)
            emotion_labels = pad_labels(emotions, max_length = max_length)
            trigger_labels = pad_labels(triggers, max_length = max_length)

            dataset = ConversationDataset(input_ids, attention_masks, emotion_labels, trigger_labels)
            loader = DataLoader(dataset, batch_size = BATCH_SIZE, shuffle = False)

            return loader

        train_loader = create_dataloader(X_train, y1_train, y2_train, max_length = MAX_LENGTH)
        val_loader = create_dataloader(X_val, y1_val, y2_val, max_length = MAX_LENGTH)
        test_loader = create_dataloader(X_test, y1_test, y2_test, max_length = MAX_LENGTH)

        """# Model configuration"""

        class EDiReFReferenceModel(nn.Module):
            def __init__(self, num_classes, model_name = 'bert-base-uncased', train_bert = True, hidden_size = 256):
                super(EDiReFReferenceModel, self).__init__()

                self.model = BertModel.from_pretrained(model_name)
                for param in self.model.parameters():
                    param.requires_grad = train_bert  # True to fine-tune model

                self.fc = nn.Linear(self.model.config.hidden_size, hidden_size)

                self.emotion_classifier = nn.Linear(hidden_size, num_classes)
                self.trigger_classifier = nn.Linear(hidden_size, 1)

                self.dropout = nn.Dropout(p = 0.1)

            def forward(self, input_ids, attention_mask):
                model_outputs = self.model(input_ids = input_ids, attention_mask = attention_mask)
                embeddings = model_outputs.last_hidden_state
                pooled_embeddings = self.dropout(embeddings)

                fc_output = torch.relu(self.fc(pooled_embeddings))

                emotion_logits = self.emotion_classifier(fc_output)
                trigger_logits = self.trigger_classifier(fc_output).squeeze(-1)

                return emotion_logits, trigger_logits

        """# Training parameters"""

        from torch.optim import AdamW
        from torch.nn import CrossEntropyLoss, BCEWithLogitsLoss

        moe = EDiReFReferenceModel(num_classes = len(labels_to_ids), model_name = DESIGNATED_MODEL, train_bert = TRAIN_BERT)
        optimizer = AdamW(moe.parameters(), lr = LEARNING_RATE)

        pos_weight = None
        if WEIGHT_TRIGGERS and DATASET in ['MaSaC', 'MaSaC_translated']:
            flattened_triggers = [x for y in y2_train for x in y]
            num_negative_samples = len([x for x in flattened_triggers if x == 0])*0.8
            num_positive_samples = len(flattened_triggers) - num_negative_samples
            pos_weight_value = num_negative_samples / num_positive_samples
            pos_weight = torch.tensor([pos_weight_value], device = device)

        emotion_loss_fn = CrossEntropyLoss()
        trigger_loss_fn = BCEWithLogitsLoss(pos_weight = pos_weight)

        moe.to(device)

        def remove_padding(logits, labels, task):
            mask = labels != -1

            logits_flat = logits.view(-1, logits.size(-1)) if task == 'emotion' else logits.view(-1)
            labels_flat = labels.view(-1)

            logits = logits_flat[mask.view(-1)]
            labels = labels_flat[mask.view(-1)]

            return logits, labels

        def evaluate(model, val_loader):
            model.eval()
            val_loss, nb_steps = 0.0, 0
            total_emotion_preds, correct_emotion_preds = 0, 0
            total_trigger_preds, correct_trigger_preds = 0, 0
            val_logs = []

            with torch.no_grad():
                for idx, batch in enumerate(val_loader):
                    input_ids = batch['input_ids'].to(device)
                    attention_mask = batch['attention_mask'].to(device)
                    emotion_labels = batch['emotion_labels'].to(device)
                    trigger_labels = batch['trigger_labels'].to(device)

                    emotion_logits, trigger_logits = model(input_ids, attention_mask)

                    # removing padding
                    emotion_logits, emotion_labels = remove_padding(emotion_logits, emotion_labels, 'emotion')
                    trigger_logits, trigger_labels = remove_padding(trigger_logits, trigger_labels, 'trigger')

                    # calculating loss
                    emotion_loss = emotion_loss_fn(emotion_logits, emotion_labels.long())
                    trigger_loss = trigger_loss_fn(trigger_logits, trigger_labels)

                    loss = emotion_loss + trigger_loss
                    val_loss += loss.item()

                    # calculating accuracy
                    emotion_preds = torch.argmax(emotion_logits, dim=-1)
                    trigger_preds = (torch.sigmoid(trigger_logits).squeeze(-1) > 0.5).long()

                    correct_emotion_preds += torch.sum(emotion_preds == emotion_labels).item()
                    correct_trigger_preds += torch.sum(trigger_preds == trigger_labels).item()

                    total_emotion_preds += emotion_labels.numel()
                    total_trigger_preds += trigger_labels.numel()

                    nb_steps += 1

                    if idx % 100 == 0:
                        loss_step = val_loss / nb_steps
                        val_logs.append(f'      Validation loss per 100 training steps: {loss_step}\n')

                avg_val_loss = val_loss / len(val_loader)
                emotion_accuracy = correct_emotion_preds / total_emotion_preds
                trigger_accuracy = correct_trigger_preds / total_trigger_preds
                avg_val_accuracy = (emotion_accuracy + trigger_accuracy)/2

            return avg_val_loss, avg_val_accuracy, val_logs

        def train_and_validate(model, train_loader, val_loader, num_epochs = 3):
            train_logs = []

            for epoch in range(num_epochs):
                train_logs.append(f"Epoch [{epoch + 1}/{num_epochs}]\n")
                model.train()
                train_loss, nb_steps = 0.0, 0
                total_emotion_preds, correct_emotion_preds = 0, 0
                total_trigger_preds, correct_trigger_preds = 0, 0

                for idx, batch in enumerate(train_loader):
                    optimizer.zero_grad()

                    input_ids = batch['input_ids'].to(device)
                    attention_mask = batch['attention_mask'].to(device)
                    emotion_labels = batch['emotion_labels'].to(device)
                    trigger_labels = batch['trigger_labels'].to(device)

                    emotion_logits, trigger_logits = model(input_ids, attention_mask)

                    # removing padding
                    emotion_logits, emotion_labels = remove_padding(emotion_logits, emotion_labels, 'emotion')
                    trigger_logits, trigger_labels = remove_padding(trigger_logits, trigger_labels, 'trigger')

                    # calculating loss
                    emotion_loss = emotion_loss_fn(emotion_logits, emotion_labels.long())
                    trigger_loss = trigger_loss_fn(trigger_logits, trigger_labels)

                    loss = emotion_loss + trigger_loss
                    train_loss += loss.item()

                    loss.backward()
                    optimizer.step()

                    # calculating accuracy
                    emotion_preds = torch.argmax(emotion_logits, dim=-1)
                    trigger_preds = (torch.sigmoid(trigger_logits).squeeze(-1) > 0.5).long()

                    correct_emotion_preds += torch.sum(emotion_preds == emotion_labels).item()
                    correct_trigger_preds += torch.sum(trigger_preds == trigger_labels).item()

                    total_emotion_preds += emotion_labels.numel()
                    total_trigger_preds += trigger_labels.numel()
                    nb_steps += 1

                    if idx % 100 == 0:
                        loss_step = train_loss / nb_steps
                        train_logs.append(f'      Training loss per 100 training steps: {loss_step}\n')

                avg_train_loss = train_loss / len(train_loader)
                emotion_accuracy = correct_emotion_preds / total_emotion_preds
                trigger_accuracy = correct_trigger_preds / total_trigger_preds
                avg_train_accuracy = (emotion_accuracy + trigger_accuracy)/2

                val_loss, val_accuracy, val_logs = evaluate(model, val_loader)
                train_logs.extend(val_logs)

                train_logs.append(f"   Training Loss: {avg_train_loss:.3f}, Training Accuracy: {avg_train_accuracy:.3f}\n")
                train_logs.append(f"   Validation Loss: {val_loss:.3f}, Validation Accuracy: {val_accuracy:.3f}\n\n")

            return train_logs

        logs = train_and_validate(moe, train_loader, val_loader, num_epochs = NUM_EPOCHS)

        from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score, confusion_matrix
        from collections import defaultdict

        def get_metrics(model, data_loader, dev):
            model.eval()

            emotion_accuracy = 0.0
            emotion_precision = 0.0
            emotion_recall = 0.0
            emotion_f1 = 0.0
            emotion_cm = None

            unique_emotions_f1 = dict.fromkeys(labels_to_ids.keys(), 0.0)

            trigger_accuracy = 0.0
            trigger_precision = 0.0
            trigger_recall = 0.0
            trigger_f1 = 0.0
            trigger_cm = None

            nb_steps = 0

            for batch in data_loader:
                input_ids = batch['input_ids'].to(dev)
                attention_mask = batch['attention_mask'].to(dev)
                emotion_labels = batch['emotion_labels'].to(dev)
                trigger_labels = batch['trigger_labels'].to(dev)

                with torch.no_grad():
                    # Forward pass
                    emotion_logits, trigger_logits = model(input_ids, attention_mask)

                    # Compute predictions for emotions
                    emotion_logits, emotion_labels = remove_padding(emotion_logits, emotion_labels, 'emotion')

                    emotion_preds = torch.argmax(emotion_logits, dim = -1)

                    emotion_preds_flat = emotion_preds.cpu().numpy()
                    emotion_labels_flat = emotion_labels.cpu().numpy()

                    # Compute predictions for triggers
                    trigger_logits, trigger_labels = remove_padding(trigger_logits, trigger_labels, 'trigger')

                    trigger_preds = (torch.sigmoid(trigger_logits).squeeze(-1) > 0.5).long()

                    trigger_preds_flat = trigger_preds.cpu().numpy()
                    trigger_labels_flat = trigger_labels.cpu().numpy()

                    # Calculate metrics for emotion classification
                    emotion_accuracy += accuracy_score(emotion_labels_flat, emotion_preds_flat)
                    emotion_precision += precision_score(emotion_labels_flat, emotion_preds_flat, average='weighted', zero_division = 0)
                    emotion_recall += recall_score(emotion_labels_flat, emotion_preds_flat, average='weighted', zero_division = 0)
                    emotion_f1 += f1_score(emotion_labels_flat, emotion_preds_flat, average='weighted', zero_division = 0)

                    for idx, score in enumerate(f1_score(emotion_labels_flat, emotion_preds_flat, average = None, zero_division = 0)):
                        unique_emotions_f1[ids_to_labels[idx]] += score

                    if emotion_cm is None:
                        emotion_cm = confusion_matrix(emotion_labels_flat, emotion_preds_flat, labels = range(len(labels_to_ids)))
                    else:
                        emotion_cm += confusion_matrix(emotion_labels_flat, emotion_preds_flat, labels = range(len(labels_to_ids)))

                    # Calculate metrics for trigger classification
                    trigger_accuracy += accuracy_score(trigger_labels_flat, trigger_preds_flat)
                    trigger_precision += precision_score(trigger_labels_flat, trigger_preds_flat, average='weighted', zero_division = 0)
                    trigger_recall += recall_score(trigger_labels_flat, trigger_preds_flat, average='weighted', zero_division = 0)
                    trigger_f1 += f1_score(trigger_labels_flat, trigger_preds_flat, average='weighted', zero_division = 0)

                    if trigger_cm is None:
                        trigger_cm = confusion_matrix(trigger_labels_flat, trigger_preds_flat, labels = [0, 1])
                    else:
                        trigger_cm += confusion_matrix(trigger_labels_flat, trigger_preds_flat, labels = [0, 1])

                    nb_steps += 1

            metrics = defaultdict(lambda: {})

            # Calculate average metrics
            avg_emotion_accuracy = emotion_accuracy / nb_steps
            avg_emotion_precision = emotion_precision / nb_steps
            avg_emotion_recall = emotion_recall / nb_steps
            avg_emotion_f1 = emotion_f1 / nb_steps

            for key, value in unique_emotions_f1.items():
                unique_emotions_f1[key] = value / nb_steps

            metrics['emotion_classification'] = {'accuracy': avg_emotion_accuracy,
                                                'precision': avg_emotion_precision,
                                                'recall': avg_emotion_recall,
                                                'f1': {'avg': avg_emotion_f1}}
            metrics['emotion_classification']['f1'].update(unique_emotions_f1)

            avg_trigger_accuracy = trigger_accuracy / nb_steps
            avg_trigger_precision = trigger_precision / nb_steps
            avg_trigger_recall = trigger_recall / nb_steps
            avg_trigger_f1 = trigger_f1 / nb_steps

            metrics['trigger_classification'] = {'accuracy': avg_trigger_accuracy,
                                                'precision': avg_trigger_precision,
                                                'recall': avg_trigger_recall,
                                                'f1': avg_trigger_f1}

            return metrics, emotion_cm, trigger_cm

        metrics, emotion_cm, trigger_cm = get_metrics(moe, test_loader, device)

        """# Save experiment"""

        def write_confusion_matrix(title, cm, labels):
            cm2txt = []
            col_width = max(len(max(labels, key=len)), 10)
            cm2txt.append('\n' + title + '\n')

            header = f"{'':<{col_width}}"
            header += "".join([f"{label:<{col_width}}" for label in labels])

            cm2txt.append(header + '\n')

            rows = []
            for i, label in enumerate(labels):
                row = f"{label:<{col_width}}"
                row += "".join([f"{value:<{col_width}}" for value in cm[i]])
                rows.append(row)

            cm2txt.append("\n".join(rows))

            return cm2txt

        import os

        if not os.path.exists(f'results/{DATASET}'):
            os.makedirs(f'results/{DATASET}')

        if WEIGHT_TRIGGERS and DATASET in ['MaSaC', 'MaSaC_translated']:
            file_path = f'results/{DATASET}/weighted_reference_model_{TRAIN_BERT}_train_bert_{LEARNING_RATE}_lr_{NUM_EPOCHS}_epochs.txt'
        else:
            file_path = f'results/{DATASET}/reference_model_{TRAIN_BERT}_train_bert_{LEARNING_RATE}_lr_{NUM_EPOCHS}_epochs.txt'

        with open(file_path, 'w') as f:
            experiment_setup = [f'MAX_LENGTH = {MAX_LENGTH}\n',
                                f'BATCH_SIZE = {BATCH_SIZE}\n', '\n',
                                f'LEARNING_RATE = {LEARNING_RATE}\n',
                                f'NUM_EPOCHS = {NUM_EPOCHS}\n',
                                f'TRAIN_BERT = {TRAIN_BERT}\n',
                                f'WEIGHT_TRIGGERS = {WEIGHT_TRIGGERS}\n', '\n', '\n']

            experiment_results = []
            for task, results in metrics.items():
                experiment_results.append(f'\nTask: {task}\n')
                for metric, score in results.items():
                    if metric != 'f1' or (metric == 'f1' and isinstance(score, (float, np.floating))):
                        experiment_results.append(f'      {metric}: {score:.3f}\n')
                    else:
                        experiment_results.append(f'      f1:\n')
                        for x, y in score.items():
                            experiment_results.append(f'          {x}: {y:.3f}\n')

            f.writelines(experiment_setup)
            f.writelines(logs)
            f.writelines(experiment_results)
            f.write("\n")
            writeable_emotion_cm = write_confusion_matrix('Emotion confusion matrix', emotion_cm, [i for i in labels_to_ids.keys()])
            f.writelines(writeable_emotion_cm)
            f.write("\n")
            writeable_trigger_cm = write_confusion_matrix('Trigger confusion matrix', trigger_cm, ['No trigger', 'Trigger'])
            f.writelines(writeable_trigger_cm)
